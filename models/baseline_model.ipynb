{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_loading as ldr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseModel(nn.Module):\n",
    "    def __init__(self, inShape: tuple):\n",
    "        super().__init__()\n",
    "        h1 = 150\n",
    "        h2 = 25\n",
    "        h3 = 2\n",
    "        self.lin1 = nn.Linear(inShape, h1)\n",
    "        self.lin2 = nn.Linear(h1, h2)\n",
    "        self.lin3 = nn.Linear(h2, h3)\n",
    "        self.dp = nn.Dropout(.20)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optim = optim.SGD(self.parameters(), lr=.01)\n",
    "        self.optim.zero_grad()\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.lin1(x)\n",
    "        y = self.dp(y)\n",
    "        y = self.tanh(y)\n",
    "        y = self.lin2(y)\n",
    "        y = self.dp(y)\n",
    "        y = self.tanh(y)\n",
    "        y = self.lin3(y)\n",
    "        y = self.softmax(y)\n",
    "        y = y[None, :]\n",
    "        return y\n",
    "\n",
    "    def step(self, y, yhat):\n",
    "        loss = self.loss(yhat, y)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data processed by very sparse matrix projections?\n",
    "useSparse = False\n",
    "epochs = 100\n",
    "dtype = torch.float32\n",
    "\n",
    "# NOTE: The data generation scripts can be found in utils/gen_data.ipynb\n",
    "if useSparse:\n",
    "    dataShape = 4350\n",
    "    sm = ldr.SmartLoader(3, load_type='restData_meanTime_spr.pt', data_shape=dataShape, dtype=dtype)\n",
    "else:\n",
    "    dataShape = 128*128*49\n",
    "    sm = ldr.SmartLoader(5, load_type='restData_meanTime_reg.pt', data_shape=dataShape, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 0\n",
      "==========================\n",
      "Finished loading training data.\n",
      "((0, 1)): Running loss: 0.7015975206159055\n",
      "((0, 2)): Running loss: 0.7126651559956372\n",
      "((0, 3)): Running loss: 0.6974553624168038\n",
      "((0, 4)): Running loss: 0.7021387284621596\n",
      "((0, 5)): Running loss: 0.6918039773590863\n",
      "((0, 6)): Running loss: 0.6934313192032278\n",
      "((0, 7)): Running loss: 0.6884957859292626\n",
      "((0, 8)): Running loss: 0.6852442733943462\n",
      "((0, 9)): Running loss: 0.6728845592588186\n",
      "((0, 10)): Running loss: 0.6945344218984246\n",
      "((0, 11)): Running loss: 0.6882271063514054\n",
      "((0, 12)): Running loss: 0.6991545609198511\n",
      "((0, 13)): Running loss: 0.6919162925332785\n",
      "((0, 14)): Running loss: 0.6970834960229695\n",
      "((0, 15)): Running loss: 0.7060211875941604\n",
      "((0, 16)): Running loss: 0.6978462710976601\n",
      "((0, 17)): Running loss: 0.7001422643661499\n",
      "((0, 18)): Running loss: 0.6919081793166697\n",
      "((0, 19)): Running loss: 0.6856463910080492\n",
      "((0, 20)): Running loss: 0.6855084931012243\n",
      "((0, 21)): Running loss: 0.6846971604973078\n",
      "((0, 22)): Running loss: 0.693307991605252\n",
      "((0, 23)): Running loss: 0.6891357223503292\n",
      "((0, 24)): Running loss: 0.6895908624865115\n",
      "((0, 25)): Running loss: 0.6804768652655184\n",
      "((0, 26)): Running loss: 0.6849402745719999\n",
      "((0, 27)): Running loss: 0.6884522000327706\n",
      "((0, 28)): Running loss: 0.6884654031600803\n",
      "((0, 29)): Running loss: 0.6846088243182749\n",
      "((0, 30)): Running loss: 0.686580492882058\n",
      "((0, 31)): Running loss: 0.6839680387638509\n",
      "((0, 32)): Running loss: 0.682867017807439\n",
      "((0, 33)): Running loss: 0.6923111961223185\n",
      "((0, 34)): Running loss: 0.6794231776148081\n",
      "((0, 35)): Running loss: 0.6818329957313836\n",
      "((0, 36)): Running loss: 0.6712647317908704\n",
      "((0, 37)): Running loss: 0.6732449454721063\n",
      "((0, 38)): Running loss: 0.6859801039099693\n",
      "((0, 39)): Running loss: 0.6659824927337468\n",
      "((0, 40)): Running loss: 0.6800103751011193\n",
      "((0, 41)): Running loss: 0.6579374114517123\n",
      "((0, 42)): Running loss: 0.6701113972812891\n",
      "((0, 43)): Running loss: 0.656658144434914\n",
      "((0, 44)): Running loss: 0.6779202679172158\n",
      "((0, 45)): Running loss: 0.6779441307298839\n",
      "((0, 46)): Running loss: 0.6856239910703152\n",
      "((0, 47)): Running loss: 0.6877156884875149\n",
      "((0, 48)): Running loss: 0.6673027449287474\n",
      "((0, 49)): Running loss: 0.6695219646207988\n",
      "((0, 50)): Running loss: 0.6898889350704849\n",
      "((0, 51)): Running loss: 0.6673737617675215\n",
      "((0, 52)): Running loss: 0.6869552147109061\n",
      "((0, 53)): Running loss: 0.6673218396026641\n",
      "((0, 54)): Running loss: 0.6669025083538145\n",
      "((0, 55)): Running loss: 0.6784132369793952\n",
      "((0, 56)): Running loss: 0.6798207319807261\n",
      "((0, 57)): Running loss: 0.6542127053253353\n",
      "((0, 58)): Running loss: 0.6787425752263516\n",
      "((0, 59)): Running loss: 0.6437632036395371\n",
      "((0, 60)): Running loss: 0.6820201799273491\n",
      "((0, 61)): Running loss: 0.6699463198892772\n",
      "((0, 62)): Running loss: 0.6743864065501839\n",
      "((0, 63)): Running loss: 0.6579280823934823\n",
      "((0, 64)): Running loss: 0.6555187660269439\n",
      "((0, 65)): Running loss: 0.6840881241951138\n",
      "((0, 66)): Running loss: 0.6624044964555651\n",
      "((0, 67)): Running loss: 0.651362820295617\n",
      "((0, 68)): Running loss: 0.66906513646245\n",
      "((0, 69)): Running loss: 0.6885539183858782\n",
      "((0, 70)): Running loss: 0.661047364352271\n",
      "((0, 71)): Running loss: 0.6846439023502171\n",
      "((0, 72)): Running loss: 0.6574110751971602\n",
      "((0, 73)): Running loss: 0.674310028553009\n",
      "((0, 74)): Running loss: 0.6755074737593532\n",
      "((0, 75)): Running loss: 0.6570429119747132\n",
      "((0, 76)): Running loss: 0.6897281291894615\n",
      "((0, 77)): Running loss: 0.6426233281381428\n",
      "((0, 78)): Running loss: 0.678769230376929\n",
      "((0, 79)): Running loss: 0.6532425936311483\n",
      "((0, 80)): Running loss: 0.6835905390325934\n",
      "((0, 81)): Running loss: 0.6700545053463429\n",
      "((0, 82)): Running loss: 0.6481583709828556\n",
      "((0, 83)): Running loss: 0.6676944866776466\n",
      "((0, 84)): Running loss: 0.6582562916446477\n",
      "((0, 85)): Running loss: 0.6834688331000507\n",
      "((0, 86)): Running loss: 0.659947267267853\n",
      "((0, 87)): Running loss: 0.6691623744554818\n",
      "((0, 88)): Running loss: 0.6496947312261909\n",
      "((0, 89)): Running loss: 0.663458323571831\n",
      "((0, 90)): Running loss: 0.7011690626386553\n",
      "((0, 91)): Running loss: 0.6610973943024874\n",
      "((0, 92)): Running loss: 0.6683433477301151\n",
      "((0, 93)): Running loss: 0.6593169879633933\n",
      "((0, 94)): Running loss: 0.6711564371362329\n",
      "((0, 95)): Running loss: 0.6748255307320505\n",
      "((0, 96)): Running loss: 0.6753546763211489\n",
      "((0, 97)): Running loss: 0.6435441551730037\n",
      "((0, 98)): Running loss: 0.6551409037783742\n",
      "((0, 99)): Running loss: 0.6578363315202296\n",
      "((0, 100)): Running loss: 0.6675782317761332\n",
      "Fold 0 Results\n",
      "--------------------------\n",
      "(0) Train Accuracy: 62.5\n",
      "(0) Test Accuracy: 56.25\n",
      "==========================\n",
      "Starting fold 1\n",
      "==========================\n",
      "Finished loading training data.\n",
      "((1, 1)): Running loss: 0.7005502781830728\n",
      "((1, 2)): Running loss: 0.6990912868641317\n",
      "((1, 3)): Running loss: 0.7030159155838192\n",
      "((1, 4)): Running loss: 0.7010532314889133\n",
      "((1, 5)): Running loss: 0.6961685125716031\n",
      "((1, 6)): Running loss: 0.692226205021143\n",
      "((1, 7)): Running loss: 0.6866130731068552\n",
      "((1, 8)): Running loss: 0.7017086781561375\n",
      "((1, 9)): Running loss: 0.6986390855163336\n",
      "((1, 10)): Running loss: 0.6909355507232249\n",
      "((1, 11)): Running loss: 0.6922543831169605\n",
      "((1, 12)): Running loss: 0.6841084347106516\n",
      "((1, 13)): Running loss: 0.6948167574591935\n",
      "((1, 14)): Running loss: 0.6868073847144842\n",
      "((1, 15)): Running loss: 0.6918558781035244\n",
      "((1, 16)): Running loss: 0.6834555950481445\n",
      "((1, 17)): Running loss: 0.6826766519807279\n",
      "((1, 18)): Running loss: 0.6846364755183458\n",
      "((1, 19)): Running loss: 0.6721669510006905\n",
      "((1, 20)): Running loss: 0.6762981254141778\n",
      "((1, 21)): Running loss: 0.6865040212869644\n",
      "((1, 22)): Running loss: 0.6732927183620632\n",
      "((1, 23)): Running loss: 0.6842768013011664\n",
      "((1, 24)): Running loss: 0.6689571030437946\n",
      "((1, 25)): Running loss: 0.6770983927417547\n",
      "((1, 26)): Running loss: 0.6750359358265996\n",
      "((1, 27)): Running loss: 0.6668994564097375\n",
      "((1, 28)): Running loss: 0.6808987250551581\n",
      "((1, 29)): Running loss: 0.6822274557780474\n",
      "((1, 30)): Running loss: 0.6570356024894863\n",
      "((1, 31)): Running loss: 0.6777417049743235\n",
      "((1, 32)): Running loss: 0.6714974343776703\n",
      "((1, 33)): Running loss: 0.6658404064364731\n",
      "((1, 34)): Running loss: 0.6700353042688221\n",
      "((1, 35)): Running loss: 0.6668033585883677\n",
      "((1, 36)): Running loss: 0.6632600098382682\n",
      "((1, 37)): Running loss: 0.6596600846387446\n",
      "((1, 38)): Running loss: 0.6727514795493335\n",
      "((1, 39)): Running loss: 0.6828607476782054\n",
      "((1, 40)): Running loss: 0.6553048407658935\n",
      "((1, 41)): Running loss: 0.6496106488630176\n",
      "((1, 42)): Running loss: 0.6693424773402512\n",
      "((1, 43)): Running loss: 0.679362352238968\n",
      "((1, 44)): Running loss: 0.6730575354304165\n",
      "((1, 45)): Running loss: 0.67789999069646\n",
      "((1, 46)): Running loss: 0.6486787707544863\n",
      "((1, 47)): Running loss: 0.6830818715970963\n",
      "((1, 48)): Running loss: 0.6696887242142111\n",
      "((1, 49)): Running loss: 0.66360331652686\n",
      "((1, 50)): Running loss: 0.6782693089917302\n",
      "((1, 51)): Running loss: 0.6569126702379435\n",
      "((1, 52)): Running loss: 0.6680421838536859\n",
      "((1, 53)): Running loss: 0.6711210978683084\n",
      "((1, 54)): Running loss: 0.6495607849210501\n",
      "((1, 55)): Running loss: 0.6535193766467273\n",
      "((1, 56)): Running loss: 0.6537320669740438\n",
      "((1, 57)): Running loss: 0.654162295628339\n",
      "((1, 58)): Running loss: 0.6438354956917465\n",
      "((1, 59)): Running loss: 0.6785573135130107\n",
      "((1, 60)): Running loss: 0.6549491644836962\n",
      "((1, 61)): Running loss: 0.6685774177312851\n",
      "((1, 62)): Running loss: 0.6557726310566068\n",
      "((1, 63)): Running loss: 0.6595008680596948\n",
      "((1, 64)): Running loss: 0.6565517976414412\n",
      "((1, 65)): Running loss: 0.6633548007812351\n",
      "((1, 66)): Running loss: 0.6856881924904883\n",
      "((1, 67)): Running loss: 0.6791865925770253\n",
      "((1, 68)): Running loss: 0.6900464971549809\n",
      "((1, 69)): Running loss: 0.6316817258484662\n",
      "((1, 70)): Running loss: 0.6460108978208154\n",
      "((1, 71)): Running loss: 0.6379594595637172\n",
      "((1, 72)): Running loss: 0.6593118095770478\n",
      "((1, 73)): Running loss: 0.6374580154661089\n",
      "((1, 74)): Running loss: 0.6415414104703814\n",
      "((1, 75)): Running loss: 0.632928776787594\n",
      "((1, 76)): Running loss: 0.6652038907632232\n",
      "((1, 77)): Running loss: 0.6667637061327696\n",
      "((1, 78)): Running loss: 0.6577764009125531\n",
      "((1, 79)): Running loss: 0.6425856105051935\n",
      "((1, 80)): Running loss: 0.6545412908308208\n",
      "((1, 81)): Running loss: 0.6537551751825958\n",
      "((1, 82)): Running loss: 0.6363143846392632\n",
      "((1, 83)): Running loss: 0.6465239129029214\n",
      "((1, 84)): Running loss: 0.6464573175180703\n",
      "((1, 85)): Running loss: 0.6683563850820065\n",
      "((1, 86)): Running loss: 0.6515350965783\n",
      "((1, 87)): Running loss: 0.6612502166535705\n",
      "((1, 88)): Running loss: 0.6429378867615014\n",
      "((1, 89)): Running loss: 0.6415247693657875\n",
      "((1, 90)): Running loss: 0.6455296904314309\n",
      "((1, 91)): Running loss: 0.6408369014970958\n",
      "((1, 92)): Running loss: 0.6436710860580206\n",
      "((1, 93)): Running loss: 0.6285925123374909\n",
      "((1, 94)): Running loss: 0.6466333230491728\n",
      "((1, 95)): Running loss: 0.6355852293781936\n",
      "((1, 96)): Running loss: 0.6298108075279742\n",
      "((1, 97)): Running loss: 0.6225368401501328\n",
      "((1, 98)): Running loss: 0.6636144623626024\n",
      "((1, 99)): Running loss: 0.65270134806633\n",
      "((1, 100)): Running loss: 0.647672557272017\n",
      "Fold 1 Results\n",
      "--------------------------\n",
      "(1) Train Accuracy: 65.625\n",
      "(1) Test Accuracy: 59.375\n",
      "==========================\n",
      "Starting fold 2\n",
      "==========================\n",
      "Finished loading training data.\n",
      "((2, 1)): Running loss: 0.7018316863104701\n",
      "((2, 2)): Running loss: 0.7039360874332488\n",
      "((2, 3)): Running loss: 0.695075923576951\n",
      "((2, 4)): Running loss: 0.6967861629091203\n",
      "((2, 5)): Running loss: 0.6943996232002974\n",
      "((2, 6)): Running loss: 0.7026163642294705\n",
      "((2, 7)): Running loss: 0.6903054085560143\n",
      "((2, 8)): Running loss: 0.6927746194414794\n",
      "((2, 9)): Running loss: 0.6958930976688862\n",
      "((2, 10)): Running loss: 0.6893589827232063\n",
      "((2, 11)): Running loss: 0.6813212744891644\n",
      "((2, 12)): Running loss: 0.6889957797247916\n",
      "((2, 13)): Running loss: 0.6895119175314903\n",
      "((2, 14)): Running loss: 0.6875951301772147\n",
      "((2, 15)): Running loss: 0.6830995748750865\n",
      "((2, 16)): Running loss: 0.6903431594837457\n",
      "((2, 17)): Running loss: 0.6845967064145952\n",
      "((2, 18)): Running loss: 0.6806292373221368\n",
      "((2, 19)): Running loss: 0.6876852009445429\n",
      "((2, 20)): Running loss: 0.6851842959877104\n",
      "((2, 21)): Running loss: 0.6794916943181306\n",
      "((2, 22)): Running loss: 0.6784635765943676\n",
      "((2, 23)): Running loss: 0.6744761592708528\n",
      "((2, 24)): Running loss: 0.6757537594530731\n",
      "((2, 25)): Running loss: 0.6669891008641571\n",
      "((2, 26)): Running loss: 0.6778455052990466\n",
      "((2, 27)): Running loss: 0.6766474356409162\n",
      "((2, 28)): Running loss: 0.6790362792089581\n",
      "((2, 29)): Running loss: 0.6719079287722707\n",
      "((2, 30)): Running loss: 0.6950567353051156\n",
      "((2, 31)): Running loss: 0.6848085278179497\n",
      "((2, 32)): Running loss: 0.6869018159341067\n",
      "((2, 33)): Running loss: 0.6704945650417358\n",
      "((2, 34)): Running loss: 0.6689337373245507\n",
      "((2, 35)): Running loss: 0.6707463033962995\n",
      "((2, 36)): Running loss: 0.6709186169318855\n",
      "((2, 37)): Running loss: 0.6622857502661645\n",
      "((2, 38)): Running loss: 0.6900184529367834\n",
      "((2, 39)): Running loss: 0.6724724867381155\n",
      "((2, 40)): Running loss: 0.6656774780713022\n",
      "((2, 41)): Running loss: 0.6505601345561445\n",
      "((2, 42)): Running loss: 0.6813583567272872\n",
      "((2, 43)): Running loss: 0.6726117115467787\n",
      "((2, 44)): Running loss: 0.6625824444927275\n",
      "((2, 45)): Running loss: 0.6708389341365546\n",
      "((2, 46)): Running loss: 0.6721959258429706\n",
      "((2, 47)): Running loss: 0.674382186261937\n",
      "((2, 48)): Running loss: 0.6843362615909427\n",
      "((2, 49)): Running loss: 0.6699274813290685\n",
      "((2, 50)): Running loss: 0.6593230876605958\n",
      "((2, 51)): Running loss: 0.6658562810625881\n",
      "((2, 52)): Running loss: 0.6555436493363231\n",
      "((2, 53)): Running loss: 0.658337689936161\n",
      "((2, 54)): Running loss: 0.653057990828529\n",
      "((2, 55)): Running loss: 0.674279950093478\n",
      "((2, 56)): Running loss: 0.679696757812053\n",
      "((2, 57)): Running loss: 0.683757203631103\n",
      "((2, 58)): Running loss: 0.6585473485756665\n",
      "((2, 59)): Running loss: 0.6913609583862126\n",
      "((2, 60)): Running loss: 0.677838028408587\n",
      "((2, 61)): Running loss: 0.6700056069530547\n",
      "((2, 62)): Running loss: 0.6533408802933991\n",
      "((2, 63)): Running loss: 0.6644319137558341\n",
      "((2, 64)): Running loss: 0.6886238467413932\n",
      "((2, 65)): Running loss: 0.6732288231141865\n",
      "((2, 66)): Running loss: 0.6861385749652982\n",
      "((2, 67)): Running loss: 0.6396311931312084\n",
      "((2, 68)): Running loss: 0.6602139654569328\n",
      "((2, 69)): Running loss: 0.6495251944288611\n",
      "((2, 70)): Running loss: 0.6586462871637195\n",
      "((2, 71)): Running loss: 0.6883814309258014\n",
      "((2, 72)): Running loss: 0.6725122791249305\n",
      "((2, 73)): Running loss: 0.6629935102537274\n",
      "((2, 74)): Running loss: 0.6421244267839938\n",
      "((2, 75)): Running loss: 0.6536539064254612\n",
      "((2, 76)): Running loss: 0.6634372691623867\n",
      "((2, 77)): Running loss: 0.6604751341510564\n",
      "((2, 78)): Running loss: 0.6736036213114858\n",
      "((2, 79)): Running loss: 0.6741303240414709\n",
      "((2, 80)): Running loss: 0.6732984397094697\n",
      "((2, 81)): Running loss: 0.644781812094152\n",
      "((2, 82)): Running loss: 0.696840092074126\n",
      "((2, 83)): Running loss: 0.6628341586329043\n",
      "((2, 84)): Running loss: 0.6548820505850017\n",
      "((2, 85)): Running loss: 0.6767569680232555\n",
      "((2, 86)): Running loss: 0.6724903709255159\n",
      "((2, 87)): Running loss: 0.6701504404190928\n",
      "((2, 88)): Running loss: 0.6766679075080901\n",
      "((2, 89)): Running loss: 0.6418903856538236\n",
      "((2, 90)): Running loss: 0.6793928660918027\n",
      "((2, 91)): Running loss: 0.6335726438555866\n",
      "((2, 92)): Running loss: 0.6675024465657771\n",
      "((2, 93)): Running loss: 0.6633987112436444\n",
      "((2, 94)): Running loss: 0.6661396254785359\n",
      "((2, 95)): Running loss: 0.6731219233479351\n",
      "((2, 96)): Running loss: 0.6667662966065109\n",
      "((2, 97)): Running loss: 0.663567379117012\n",
      "((2, 98)): Running loss: 0.6203132832888514\n",
      "((2, 99)): Running loss: 0.6845800615847111\n",
      "((2, 100)): Running loss: 0.6657240882050246\n",
      "Fold 2 Results\n",
      "--------------------------\n",
      "(2) Train Accuracy: 67.96875\n",
      "(2) Test Accuracy: 59.375\n",
      "==========================\n",
      "Starting fold 3\n",
      "==========================\n",
      "Finished loading training data.\n",
      "((3, 1)): Running loss: 0.7066344586201012\n",
      "((3, 2)): Running loss: 0.7028406159952283\n",
      "((3, 3)): Running loss: 0.6974754901602864\n",
      "((3, 4)): Running loss: 0.699212749954313\n",
      "((3, 5)): Running loss: 0.6938412226736546\n",
      "((3, 6)): Running loss: 0.692437369376421\n",
      "((3, 7)): Running loss: 0.69703602604568\n",
      "((3, 8)): Running loss: 0.7002837699837983\n",
      "((3, 9)): Running loss: 0.6935330359265208\n",
      "((3, 10)): Running loss: 0.696403727401048\n",
      "((3, 11)): Running loss: 0.6999881854280829\n",
      "((3, 12)): Running loss: 0.7032425366342068\n",
      "((3, 13)): Running loss: 0.7082325196824968\n",
      "((3, 14)): Running loss: 0.6963184722699225\n",
      "((3, 15)): Running loss: 0.6921907318755984\n",
      "((3, 16)): Running loss: 0.6863107038661838\n",
      "((3, 17)): Running loss: 0.7012790427543223\n",
      "((3, 18)): Running loss: 0.6945267496630549\n",
      "((3, 19)): Running loss: 0.6996366041712463\n",
      "((3, 20)): Running loss: 0.695732282474637\n",
      "((3, 21)): Running loss: 0.6941629801876843\n",
      "((3, 22)): Running loss: 0.6873875055462122\n",
      "((3, 23)): Running loss: 0.6894877511076629\n",
      "((3, 24)): Running loss: 0.691023032180965\n",
      "((3, 25)): Running loss: 0.6892599728889763\n",
      "((3, 26)): Running loss: 0.6995554585009813\n",
      "((3, 27)): Running loss: 0.6986292297951877\n",
      "((3, 28)): Running loss: 0.693019294179976\n",
      "((3, 29)): Running loss: 0.6868654449936002\n",
      "((3, 30)): Running loss: 0.6909978985786438\n",
      "((3, 31)): Running loss: 0.6937453262507915\n",
      "((3, 32)): Running loss: 0.6907784244976938\n",
      "((3, 33)): Running loss: 0.6876936317421496\n",
      "((3, 34)): Running loss: 0.6849153744988143\n",
      "((3, 35)): Running loss: 0.7005716096609831\n",
      "((3, 36)): Running loss: 0.6847954976838082\n",
      "((3, 37)): Running loss: 0.6919964291155338\n",
      "((3, 38)): Running loss: 0.679974316386506\n",
      "((3, 39)): Running loss: 0.6844276960473508\n",
      "((3, 40)): Running loss: 0.6978530692867935\n",
      "((3, 41)): Running loss: 0.6906916431616992\n",
      "((3, 42)): Running loss: 0.679800093639642\n",
      "((3, 43)): Running loss: 0.682171599008143\n",
      "((3, 44)): Running loss: 0.6920229503884912\n",
      "((3, 45)): Running loss: 0.6915238068904728\n",
      "((3, 46)): Running loss: 0.6947908475995064\n",
      "((3, 47)): Running loss: 0.6934869601391256\n",
      "((3, 48)): Running loss: 0.6928425817750394\n",
      "((3, 49)): Running loss: 0.6972915024962276\n",
      "((3, 50)): Running loss: 0.680256430990994\n",
      "((3, 51)): Running loss: 0.6919760929886252\n",
      "((3, 52)): Running loss: 0.6977259614504874\n",
      "((3, 53)): Running loss: 0.697039283812046\n",
      "((3, 54)): Running loss: 0.6840310476254672\n",
      "((3, 55)): Running loss: 0.6690042100381106\n",
      "((3, 56)): Running loss: 0.6694653101731092\n",
      "((3, 57)): Running loss: 0.6822719133924693\n",
      "((3, 58)): Running loss: 0.6791865869890898\n",
      "((3, 59)): Running loss: 0.6793540259823203\n",
      "((3, 60)): Running loss: 0.6717497520148754\n",
      "((3, 61)): Running loss: 0.6861523916013539\n",
      "((3, 62)): Running loss: 0.6925663906149566\n",
      "((3, 63)): Running loss: 0.668604773003608\n",
      "((3, 64)): Running loss: 0.674547258997336\n",
      "((3, 65)): Running loss: 0.6888809979427606\n",
      "((3, 66)): Running loss: 0.6830758568830788\n",
      "((3, 67)): Running loss: 0.6971993504557759\n",
      "((3, 68)): Running loss: 0.6757066424470395\n",
      "((3, 69)): Running loss: 0.6738476678729057\n",
      "((3, 70)): Running loss: 0.6909071404952556\n",
      "((3, 71)): Running loss: 0.6654399421531707\n",
      "((3, 72)): Running loss: 0.6743898850400001\n",
      "((3, 73)): Running loss: 0.6851935191079974\n",
      "((3, 74)): Running loss: 0.6830095374025404\n",
      "((3, 75)): Running loss: 0.6817376881372184\n",
      "((3, 76)): Running loss: 0.6793261324055493\n",
      "((3, 77)): Running loss: 0.6885117208585143\n",
      "((3, 78)): Running loss: 0.6654597187880427\n",
      "((3, 79)): Running loss: 0.6761543080210686\n",
      "((3, 80)): Running loss: 0.6623944658786058\n",
      "((3, 81)): Running loss: 0.6786810802295804\n",
      "((3, 82)): Running loss: 0.6672429798636585\n",
      "((3, 83)): Running loss: 0.6976316494401544\n",
      "((3, 84)): Running loss: 0.6983094264287502\n",
      "((3, 85)): Running loss: 0.6790419560857117\n",
      "((3, 86)): Running loss: 0.689391607651487\n",
      "((3, 87)): Running loss: 0.6687419470399618\n",
      "((3, 88)): Running loss: 0.6679394084494561\n",
      "((3, 89)): Running loss: 0.6782901345286518\n",
      "((3, 90)): Running loss: 0.6773232240229845\n",
      "((3, 91)): Running loss: 0.6678491807542741\n",
      "((3, 92)): Running loss: 0.6884496170096099\n",
      "((3, 93)): Running loss: 0.6691999239847064\n",
      "((3, 94)): Running loss: 0.690361566375941\n",
      "((3, 95)): Running loss: 0.6697003913577646\n",
      "((3, 96)): Running loss: 0.689701912458986\n",
      "((3, 97)): Running loss: 0.6742842006497085\n",
      "((3, 98)): Running loss: 0.6783251236192882\n",
      "((3, 99)): Running loss: 0.686131646623835\n",
      "((3, 100)): Running loss: 0.6592845928389579\n",
      "Fold 3 Results\n",
      "--------------------------\n",
      "(3) Train Accuracy: 61.71875\n",
      "(3) Test Accuracy: 62.5\n",
      "==========================\n",
      "Starting fold 4\n",
      "==========================\n",
      "Finished loading training data.\n",
      "((4, 1)): Running loss: 0.7090101167559624\n",
      "((4, 2)): Running loss: 0.7003909526392817\n",
      "((4, 3)): Running loss: 0.7130763246677816\n",
      "((4, 4)): Running loss: 0.7095104260370135\n",
      "((4, 5)): Running loss: 0.6930463640019298\n",
      "((4, 6)): Running loss: 0.7028871453367174\n",
      "((4, 7)): Running loss: 0.7025701501406729\n",
      "((4, 8)): Running loss: 0.7005014577880502\n",
      "((4, 9)): Running loss: 0.7000123816542327\n",
      "((4, 10)): Running loss: 0.6991484565660357\n",
      "((4, 11)): Running loss: 0.6953939064405859\n",
      "((4, 12)): Running loss: 0.6945465113967657\n",
      "((4, 13)): Running loss: 0.6970935137942433\n",
      "((4, 14)): Running loss: 0.693950382526964\n",
      "((4, 15)): Running loss: 0.7052008691243827\n",
      "((4, 16)): Running loss: 0.698018204420805\n",
      "((4, 17)): Running loss: 0.6942966734059155\n",
      "((4, 18)): Running loss: 0.6984320245683193\n",
      "((4, 19)): Running loss: 0.6976312673650682\n",
      "((4, 20)): Running loss: 0.6922761998139322\n",
      "((4, 21)): Running loss: 0.6870798403397202\n",
      "((4, 22)): Running loss: 0.6923713819123805\n",
      "((4, 23)): Running loss: 0.6904428792186081\n",
      "((4, 24)): Running loss: 0.6903923046775162\n",
      "((4, 25)): Running loss: 0.6904494715854526\n",
      "((4, 26)): Running loss: 0.6977126286365092\n",
      "((4, 27)): Running loss: 0.6975910859182477\n",
      "((4, 28)): Running loss: 0.691777462605387\n",
      "((4, 29)): Running loss: 0.6845910656265914\n",
      "((4, 30)): Running loss: 0.6957901767455041\n",
      "((4, 31)): Running loss: 0.6941913487389684\n",
      "((4, 32)): Running loss: 0.6940017701126635\n",
      "((4, 33)): Running loss: 0.684593309648335\n",
      "((4, 34)): Running loss: 0.6896076751872897\n",
      "((4, 35)): Running loss: 0.6963422782719135\n",
      "((4, 36)): Running loss: 0.6895212305244058\n",
      "((4, 37)): Running loss: 0.6880310520064086\n",
      "((4, 38)): Running loss: 0.687989899655804\n",
      "((4, 39)): Running loss: 0.6904318241868168\n",
      "((4, 40)): Running loss: 0.6836231546476483\n",
      "((4, 41)): Running loss: 0.6817319625988603\n",
      "((4, 42)): Running loss: 0.6863730205222964\n",
      "((4, 43)): Running loss: 0.6845281717833132\n",
      "((4, 44)): Running loss: 0.6848560844082385\n",
      "((4, 45)): Running loss: 0.6906305816955864\n",
      "((4, 46)): Running loss: 0.6968542407266796\n",
      "((4, 47)): Running loss: 0.682997222756967\n",
      "((4, 48)): Running loss: 0.6794776937458664\n",
      "((4, 49)): Running loss: 0.6840755566954613\n",
      "((4, 50)): Running loss: 0.6775513906031847\n",
      "((4, 51)): Running loss: 0.6888441760092974\n",
      "((4, 52)): Running loss: 0.6774016045965254\n",
      "((4, 53)): Running loss: 0.6959001529030502\n",
      "((4, 54)): Running loss: 0.6857798299752176\n",
      "((4, 55)): Running loss: 0.6909950894769281\n",
      "((4, 56)): Running loss: 0.6827681309077889\n",
      "((4, 57)): Running loss: 0.6823502336628735\n",
      "((4, 58)): Running loss: 0.672635912662372\n",
      "((4, 59)): Running loss: 0.6854502987116575\n",
      "((4, 60)): Running loss: 0.6890090664383024\n",
      "((4, 61)): Running loss: 0.6827417972963303\n",
      "((4, 62)): Running loss: 0.7009781557135284\n",
      "((4, 63)): Running loss: 0.6875989933032542\n",
      "((4, 64)): Running loss: 0.6776242458727211\n",
      "((4, 65)): Running loss: 0.6751439298968762\n",
      "((4, 66)): Running loss: 0.6952859957236797\n",
      "((4, 67)): Running loss: 0.7003728726413101\n",
      "((4, 68)): Running loss: 0.6797273168340325\n",
      "((4, 69)): Running loss: 0.6866231400053948\n",
      "((4, 70)): Running loss: 0.6735820071771741\n",
      "((4, 71)): Running loss: 0.6712576225399971\n",
      "((4, 72)): Running loss: 0.6990947308950126\n",
      "((4, 73)): Running loss: 0.6729906685650349\n",
      "((4, 74)): Running loss: 0.6872148001566529\n",
      "((4, 75)): Running loss: 0.6477651209570467\n",
      "((4, 76)): Running loss: 0.6788217450957745\n",
      "((4, 77)): Running loss: 0.6782495197840035\n",
      "((4, 78)): Running loss: 0.6579804776702076\n",
      "((4, 79)): Running loss: 0.6553464555181563\n",
      "((4, 80)): Running loss: 0.6650825468823314\n",
      "((4, 81)): Running loss: 0.6728239459916949\n",
      "((4, 82)): Running loss: 0.6670875996351242\n",
      "((4, 83)): Running loss: 0.6545928986743093\n",
      "((4, 84)): Running loss: 0.6689880809281021\n",
      "((4, 85)): Running loss: 0.6870870445854962\n",
      "((4, 86)): Running loss: 0.6673875479027629\n",
      "((4, 87)): Running loss: 0.6875036391429603\n",
      "((4, 88)): Running loss: 0.6702741596382111\n",
      "((4, 89)): Running loss: 0.6822182249743491\n",
      "((4, 90)): Running loss: 0.6871654558926821\n",
      "((4, 91)): Running loss: 0.671281028771773\n",
      "((4, 92)): Running loss: 0.6788170244544744\n",
      "((4, 93)): Running loss: 0.6729729729704559\n",
      "((4, 94)): Running loss: 0.6728325141593814\n",
      "((4, 95)): Running loss: 0.6760691704694182\n",
      "((4, 96)): Running loss: 0.6731041714083403\n",
      "((4, 97)): Running loss: 0.6679449146613479\n",
      "((4, 98)): Running loss: 0.6660896567627788\n",
      "((4, 99)): Running loss: 0.6803995021618903\n",
      "((4, 100)): Running loss: 0.6664134771563113\n",
      "Fold 4 Results\n",
      "--------------------------\n",
      "(4) Train Accuracy: 60.9375\n",
      "(4) Test Accuracy: 50.0\n",
      "==========================\n",
      "Average Training Accuracies\n",
      "Sleep Deprived: 63.44181420850033%\tWell Rested: 64.67688870159061%\n",
      "Total: 63.75%\n",
      "\n",
      "Average Testing Accuracies\n",
      "Sleep Deprived: 61.249999996171866%\tWell Rested: 53.749999996640625%\n",
      "Total: 57.5%\n"
     ]
    }
   ],
   "source": [
    "model = baseModel(dataShape).cuda()\n",
    "sm.run(model, epochs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "259bfae0818e842f806df57aeba9e9fea25079ec5799bff4e7264e20bda53058"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
