{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291a4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant variables\n",
    "data_path = '../ds000201-download/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3824529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess\n",
    "def load_subject_and_compress(subject_name, task_name, data_shape, data_path):\n",
    "    subject_path = data_path + subject_name\n",
    "    \n",
    "    ses_1_data_path = subject_path + f'/ses-1/func/{subject_name}_ses-1_{task_name}.nii.gz'\n",
    "    ses_2_data_path = subject_path + f'/ses-2/func/{subject_name}_ses-2_{task_name}.nii.gz'\n",
    "    \n",
    "    print(ses_1_data_path)\n",
    "    try:\n",
    "        ses_1_img = nib.load(ses_1_data_path)\n",
    "        ses_2_img = nib.load(ses_2_data_path)\n",
    "    except(FileNotFoundError):\n",
    "        return None\n",
    "    \n",
    "    ses_1_data = ses_1_img.get_fdata()\n",
    "    ses_2_data = ses_2_img.get_fdata()\n",
    "    \n",
    "    print(ses_1_data.shape)\n",
    "    #Average along time axis to compress data\n",
    "    ses_1 = np.mean(ses_1_data, axis=-1)\n",
    "    ses_1 = np.pad(ses_1, [(0, data_shape[0] - ses_1_data.shape[0]), \n",
    "                           (0, data_shape[1] - ses_1_data.shape[1]), \n",
    "                           (0, data_shape[2] - ses_1_data.shape[2])])\n",
    "    \n",
    "    ses_2 = np.mean(ses_2_data, axis=-1)\n",
    "    ses_2 = np.pad(ses_2, [(0, data_shape[0] - ses_2_data.shape[0]), \n",
    "                           (0, data_shape[1] - ses_2_data.shape[1]), \n",
    "                           (0, data_shape[2] - ses_2_data.shape[2])])\n",
    "    \n",
    "    # Normalize\n",
    "    \n",
    "    ses_1 = (ses_1 - np.mean(ses_1)) / np.std(ses_1)\n",
    "    ses_2 = (ses_2 - np.mean(ses_2)) / np.std(ses_2)\n",
    "    \n",
    "    return ses_1, ses_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a3d33e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the compressed subject data for all valid subjects\n",
    "def retrieve_data(data_path, task_name, data_shape):\n",
    "    rested_data = []\n",
    "    sleep_deprived_data = []\n",
    "    \n",
    "    participants = pd.read_csv('../ds000201-download/participants.tsv', sep='\\t')\n",
    "    rows = participants.iterrows()\n",
    "    \n",
    "    for idx, row in rows:\n",
    "        pid = row['participant_id']\n",
    "        \n",
    "        compressed_subject_data = load_subject_and_compress(pid, task_name, data_shape, data_path)\n",
    "        if (compressed_subject_data != None):\n",
    "            print(compressed_subject_data[0].shape)\n",
    "            print(compressed_subject_data[1].shape)\n",
    "            \n",
    "            sd_session_idx = row['Sl_cond'] - 1\n",
    "            rested_session_idx = int(not sd_session_idx)\n",
    "            \n",
    "            rested_data.append(compressed_subject_data[rested_session_idx])\n",
    "            sleep_deprived_data.append(compressed_subject_data[sd_session_idx])\n",
    "            \n",
    "#     for root, dirs, files in os.walk(data_path):\n",
    "#         for folder_name in dirs:\n",
    "#             print(folder_name)\n",
    "#             if folder_name[:3] == 'sub' and len(folder_name) == 8:\n",
    "#                 compressed_subject_data = load_subject_and_compress(folder_name, data_path)\n",
    "#                 if (compressed_subject_data != None):\n",
    "#                     ses_1_data.append(compressed_subject_data[0])\n",
    "#                     ses_2_data.append(compressed_subject_data[1]) \n",
    "    \n",
    "    return np.array(rested_data), np.array(sleep_deprived_data)\n",
    "\n",
    "# Split data into training and test sets\n",
    "def split_training_test(rested_data, sleep_deprived_data):\n",
    "    # 0 label means rested, 1 label means sleepy\n",
    "    assert(rested_data.shape == sleep_deprived_data.shape)\n",
    "    \n",
    "    N = rested_data.shape[0]\n",
    "    \n",
    "    shuffler = np.random.permutation(N)\n",
    "    rested_shuffle = rested_data[shuffler]\n",
    "    sleep_deprived_shuffle = sleep_deprived_data[shuffler]\n",
    "    \n",
    "    num_train = int(.8 * N)\n",
    "    num_test = N - num_train\n",
    "    \n",
    "    #Split into training and test sets, keeping subjects consistent within sets\n",
    "    rested_train = rested_shuffle[:num_train]\n",
    "    rested_test = rested_shuffle[-num_test:]\n",
    "    sleep_deprived_train = sleep_deprived_shuffle[:num_train]\n",
    "    sleep_deprived_test = sleep_deprived_shuffle[-num_test:]\n",
    "    \n",
    "    #Define labels for training and test sets\n",
    "    rested_train_labels = [0 for i in range(num_train)]\n",
    "    rested_test_labels = [0 for i in range(num_test)]\n",
    "    sleep_deprived_train_labels = [1 for i in range(num_train)]\n",
    "    sleep_deprived_test_labels = [1 for i in range(num_train)]\n",
    "    \n",
    "    #zip labels up\n",
    "    rested_train_set = zip(rested_train, rested_train_labels)\n",
    "    sleep_deprived_train_set = zip(sleep_deprived_train, sleep_deprived_train_labels)\n",
    "    rested_test_set = zip(rested_test, rested_test_labels)\n",
    "    sleep_deprived_test_set =zip(sleep_deprived_test, sleep_deprived_test_labels)\n",
    "    \n",
    "    #put all training and test fMRI images into the same train set, with labels included now\n",
    "    train_set = list(rested_train_set) + list(sleep_deprived_train_set)\n",
    "    test_set = list(rested_test_set) + list(sleep_deprived_test_set)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f07b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ds000201-download/sub-9001/ses-1/func/sub-9001_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9002/ses-1/func/sub-9002_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9003/ses-1/func/sub-9003_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9004/ses-1/func/sub-9004_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9005/ses-1/func/sub-9005_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9007/ses-1/func/sub-9007_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9008/ses-1/func/sub-9008_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9009/ses-1/func/sub-9009_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9011/ses-1/func/sub-9011_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9012/ses-1/func/sub-9012_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9013/ses-1/func/sub-9013_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9014/ses-1/func/sub-9014_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9016/ses-1/func/sub-9016_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9017/ses-1/func/sub-9017_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9018/ses-1/func/sub-9018_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9019/ses-1/func/sub-9019_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9020/ses-1/func/sub-9020_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9022/ses-1/func/sub-9022_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9023/ses-1/func/sub-9023_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9025/ses-1/func/sub-9025_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9026/ses-1/func/sub-9026_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9028/ses-1/func/sub-9028_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9029/ses-1/func/sub-9029_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9032/ses-1/func/sub-9032_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9033/ses-1/func/sub-9033_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9034/ses-1/func/sub-9034_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9035/ses-1/func/sub-9035_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9036/ses-1/func/sub-9036_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9037/ses-1/func/sub-9037_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9038/ses-1/func/sub-9038_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9039/ses-1/func/sub-9039_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9040/ses-1/func/sub-9040_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9041/ses-1/func/sub-9041_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9042/ses-1/func/sub-9042_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9044/ses-1/func/sub-9044_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9045/ses-1/func/sub-9045_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9046/ses-1/func/sub-9046_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9047/ses-1/func/sub-9047_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9048/ses-1/func/sub-9048_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9049/ses-1/func/sub-9049_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9050/ses-1/func/sub-9050_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9053/ses-1/func/sub-9053_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9054/ses-1/func/sub-9054_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9055/ses-1/func/sub-9055_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9056/ses-1/func/sub-9056_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9057/ses-1/func/sub-9057_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9058/ses-1/func/sub-9058_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9059/ses-1/func/sub-9059_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9061/ses-1/func/sub-9061_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9062/ses-1/func/sub-9062_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9063/ses-1/func/sub-9063_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9064/ses-1/func/sub-9064_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9065/ses-1/func/sub-9065_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9066/ses-1/func/sub-9066_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9067/ses-1/func/sub-9067_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9068/ses-1/func/sub-9068_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9069/ses-1/func/sub-9069_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9070/ses-1/func/sub-9070_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9071/ses-1/func/sub-9071_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9072/ses-1/func/sub-9072_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9073/ses-1/func/sub-9073_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9074/ses-1/func/sub-9074_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9075/ses-1/func/sub-9075_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9076/ses-1/func/sub-9076_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9077/ses-1/func/sub-9077_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9078/ses-1/func/sub-9078_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9079/ses-1/func/sub-9079_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9080/ses-1/func/sub-9080_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9081/ses-1/func/sub-9081_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9082/ses-1/func/sub-9082_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9083/ses-1/func/sub-9083_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9084/ses-1/func/sub-9084_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9085/ses-1/func/sub-9085_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9086/ses-1/func/sub-9086_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9087/ses-1/func/sub-9087_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9088/ses-1/func/sub-9088_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9089/ses-1/func/sub-9089_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9090/ses-1/func/sub-9090_ses-1_task-rest_bold.nii.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9091/ses-1/func/sub-9091_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9092/ses-1/func/sub-9092_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9093/ses-1/func/sub-9093_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9094/ses-1/func/sub-9094_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9095/ses-1/func/sub-9095_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9096/ses-1/func/sub-9096_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9098/ses-1/func/sub-9098_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9100/ses-1/func/sub-9100_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve rest data\n",
    "ses_1_rest_task_data, ses_2_rest_task_data = retrieve_data(data_path, 'task-rest_bold', data_shape=(128,128,49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "bd04659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 8, 7, stride = 2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(4608, 128)\n",
    "        self.linear2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool3d(5, stride = 5)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.max_pool(self.conv1(x)))\n",
    "        \n",
    "        x = self.dropout(self.relu(self.linear1(torch.flatten(x, start_dim=1))))\n",
    "        x = self.softmax(self.linear2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "23c4a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDeprivationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 8, 5, stride = 2)\n",
    "        self.conv2 = nn.Conv3d(8, 16, 3, stride = 2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(1568,  128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.max_pool = nn.MaxPool3d(2, stride = 2)\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(self.conv1(x))\n",
    "        x = self.max_pool(self.conv2(x))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.linear1(torch.flatten(x, start_dim=1))))\n",
    "        #x = F.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "49c96195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate net\n",
    "is_simple_net = True\n",
    "\n",
    "if is_simple_net:\n",
    "    net = SimpleNet()\n",
    "else:\n",
    "    net = SleepDeprivationNet()\n",
    "net = net.double()\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "44a33fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 49])\n",
      "torch.Size([1, 1, 128, 128, 49])\n"
     ]
    }
   ],
   "source": [
    "# Simple sanity checks \n",
    "test = torch.from_numpy(ses_1_rest_task_data[0]).to(device)\n",
    "label = torch.tensor([0]).to(device)\n",
    "print(test.shape)\n",
    "test = test[None,None, :]\n",
    "print(test.shape)\n",
    "\n",
    "test = test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "2a0ed513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6178, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.6161, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.5057, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4705, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4143, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4582, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4696, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4101, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4112, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4124, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3809, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4283, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3948, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3629, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3900, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3715, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3424, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3624, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4141, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3534, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3617, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3314, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3320, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3323, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3369, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3649, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3435, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3916, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3257, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3226, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3455, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3410, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3218, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3472, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3352, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3761, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3241, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3398, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3420, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3251, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.4004, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3392, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3809, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3332, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3246, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3636, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3183, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3163, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3253, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3248, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3461, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3185, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3225, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3477, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3240, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3469, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3264, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3208, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3163, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3205, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3176, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3168, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3255, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3444, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3182, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3175, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3230, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3176, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3246, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3382, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3328, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3157, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3173, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3834, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3300, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3252, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3482, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3603, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3175, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3176, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3205, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3199, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3175, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3258, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3708, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3239, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3304, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3254, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3277, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.3190, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3212, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3210, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3229, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3190, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3223, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3404, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3547, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3333, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3280, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Overfit one sample for sanity\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min = 1e-6)\n",
    "\n",
    "net.train()\n",
    "\n",
    "#Save model each iteration of training\n",
    "for epoch in range(num_epochs):     \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(test)\n",
    "    \n",
    "    loss = criterion(outputs, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss\", loss)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "869b0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9947, 0.0053]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "35e61ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "train_set, test_set = split_training_test(ses_1_rest_task_data, ses_2_rest_task_data)\n",
    "\n",
    "#Shuffle train and test data\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "e762a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-5)\n",
    "num_epochs = 300\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min = 1e-6)\n",
    "                                                                                                 \n",
    "batch_size = 8\n",
    "train_len = len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "f9b4e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss 0.8032173511910429\n",
      "Epoch 1, average loss 0.80031716141364\n",
      "Epoch 2, average loss 0.8034415282570657\n",
      "Epoch 3, average loss 0.7999709536854148\n",
      "Epoch 4, average loss 0.7955011822826267\n",
      "Epoch 5, average loss 0.79531875013193\n",
      "Epoch 6, average loss 0.7926683046987656\n",
      "Epoch 7, average loss 0.7939729375349459\n",
      "Epoch 8, average loss 0.7968384791241452\n",
      "Epoch 9, average loss 0.7973933456821808\n",
      "Epoch 10, average loss 0.7967315111765081\n",
      "Epoch 11, average loss 0.8003306691785655\n",
      "Epoch 12, average loss 0.7981726208723278\n",
      "Epoch 13, average loss 0.8014327933266036\n",
      "Epoch 14, average loss 0.7952169328596264\n",
      "Epoch 15, average loss 0.7977170901780056\n",
      "Epoch 16, average loss 0.7950333522080988\n",
      "Epoch 17, average loss 0.7981245441831237\n",
      "Epoch 18, average loss 0.7988770497556574\n",
      "Epoch 19, average loss 0.7952206834391033\n",
      "Epoch 20, average loss 0.7997744621946948\n",
      "Epoch 21, average loss 0.7985757286427375\n",
      "Epoch 22, average loss 0.7981583045758058\n",
      "Epoch 23, average loss 0.8035331098637064\n",
      "Epoch 24, average loss 0.7989854158253475\n",
      "Epoch 25, average loss 0.7959068368967008\n",
      "Epoch 26, average loss 0.7971192099240726\n",
      "Epoch 27, average loss 0.7992676965260006\n",
      "Epoch 28, average loss 0.7928756433209777\n",
      "Epoch 29, average loss 0.7992821113787508\n",
      "Epoch 30, average loss 0.7993043377044362\n",
      "Epoch 31, average loss 0.7993773106512295\n",
      "Epoch 32, average loss 0.795327242380159\n",
      "Epoch 33, average loss 0.7950908963196217\n",
      "Epoch 34, average loss 0.7938908321792371\n",
      "Epoch 35, average loss 0.8000734434098026\n",
      "Epoch 36, average loss 0.7989260891303003\n",
      "Epoch 37, average loss 0.8005266779146992\n",
      "Epoch 38, average loss 0.7953741448550835\n",
      "Epoch 39, average loss 0.7970676659060097\n",
      "Epoch 40, average loss 0.8004586071703547\n",
      "Epoch 41, average loss 0.7962871843877695\n",
      "Epoch 42, average loss 0.7893575861448026\n",
      "Epoch 43, average loss 0.7983571641404553\n",
      "Epoch 44, average loss 0.7984063456454527\n",
      "Epoch 45, average loss 0.7973416071338288\n",
      "Epoch 46, average loss 0.7989347973708234\n",
      "Epoch 47, average loss 0.7988870125534143\n",
      "Epoch 48, average loss 0.7959074326766282\n",
      "Epoch 49, average loss 0.7945896521147612\n",
      "Epoch 50, average loss 0.7933108371834955\n",
      "Epoch 51, average loss 0.7920700945592916\n",
      "Epoch 52, average loss 0.7904376850848366\n",
      "Epoch 53, average loss 0.7897346898593097\n",
      "Epoch 54, average loss 0.8028782169216792\n",
      "Epoch 55, average loss 0.7995759446298473\n",
      "Epoch 56, average loss 0.7954227495819564\n",
      "Epoch 57, average loss 0.7957804586943535\n",
      "Epoch 58, average loss 0.7951460050988495\n",
      "Epoch 59, average loss 0.7947658461080384\n",
      "Epoch 60, average loss 0.7932902482133333\n",
      "Epoch 61, average loss 0.7970087672644833\n",
      "Epoch 62, average loss 0.8024177206838318\n",
      "Epoch 63, average loss 0.7970608750288177\n",
      "Epoch 64, average loss 0.7935835666859542\n",
      "Epoch 65, average loss 0.801432856015919\n",
      "Epoch 66, average loss 0.7984518676266118\n",
      "Epoch 67, average loss 0.7898354824470892\n",
      "Epoch 68, average loss 0.8000920964375211\n",
      "Epoch 69, average loss 0.7953217210096062\n",
      "Epoch 70, average loss 0.7960973560648913\n",
      "Epoch 71, average loss 0.790547797420879\n",
      "Epoch 72, average loss 0.7959549537246863\n",
      "Epoch 73, average loss 0.7945728959835288\n",
      "Epoch 74, average loss 0.7976013807278985\n",
      "Epoch 75, average loss 0.7901081160851964\n",
      "Epoch 76, average loss 0.7994446729104328\n",
      "Epoch 77, average loss 0.8011019477922094\n",
      "Epoch 78, average loss 0.8012388856630991\n",
      "Epoch 79, average loss 0.7989583456563789\n",
      "Epoch 80, average loss 0.795465253819175\n",
      "Epoch 81, average loss 0.7938121162963432\n",
      "Epoch 82, average loss 0.7904969013866607\n",
      "Epoch 83, average loss 0.7932913862015277\n",
      "Epoch 84, average loss 0.7965375685917012\n",
      "Epoch 85, average loss 0.7917704544681704\n",
      "Epoch 86, average loss 0.7896834856726958\n",
      "Epoch 87, average loss 0.7902610311096733\n",
      "Epoch 88, average loss 0.7993279468226963\n",
      "Epoch 89, average loss 0.7903378703458823\n",
      "Epoch 90, average loss 0.7961425564897857\n",
      "Epoch 91, average loss 0.7938079845450184\n",
      "Epoch 92, average loss 0.7978206256048206\n",
      "Epoch 93, average loss 0.7911496769645485\n",
      "Epoch 94, average loss 0.7916307269413565\n",
      "Epoch 95, average loss 0.7917928837836004\n",
      "Epoch 96, average loss 0.7941258096235776\n",
      "Epoch 97, average loss 0.7913871940074642\n",
      "Epoch 98, average loss 0.7949880002996198\n",
      "Epoch 99, average loss 0.7920282153816006\n",
      "Epoch 100, average loss 0.7921403100917724\n",
      "Epoch 101, average loss 0.7872541237002534\n",
      "Epoch 102, average loss 0.796044066776531\n",
      "Epoch 103, average loss 0.7830143757322666\n",
      "Epoch 104, average loss 0.7902219581901756\n",
      "Epoch 105, average loss 0.7951751214349355\n",
      "Epoch 106, average loss 0.7928519788333547\n",
      "Epoch 107, average loss 0.7967626593182796\n",
      "Epoch 108, average loss 0.7878416276518576\n",
      "Epoch 109, average loss 0.7902510314881622\n",
      "Epoch 110, average loss 0.8011864756887949\n",
      "Epoch 111, average loss 0.7977870724490963\n",
      "Epoch 112, average loss 0.7938610050696221\n",
      "Epoch 113, average loss 0.7952132862762719\n",
      "Epoch 114, average loss 0.7961554013597009\n",
      "Epoch 115, average loss 0.7906577893205382\n",
      "Epoch 116, average loss 0.7999760357704855\n",
      "Epoch 117, average loss 0.799157423654288\n",
      "Epoch 118, average loss 0.7883323511045055\n",
      "Epoch 119, average loss 0.7934208454376073\n",
      "Epoch 120, average loss 0.7904700465874527\n",
      "Epoch 121, average loss 0.7934687644451024\n",
      "Epoch 122, average loss 0.7969781379667836\n",
      "Epoch 123, average loss 0.7931305822576233\n",
      "Epoch 124, average loss 0.7860743813094189\n",
      "Epoch 125, average loss 0.7911222949301461\n",
      "Epoch 126, average loss 0.7956676367724395\n",
      "Epoch 127, average loss 0.7844302740574461\n",
      "Epoch 128, average loss 0.7957307821019332\n",
      "Epoch 129, average loss 0.794668724560216\n",
      "Epoch 130, average loss 0.7895214546108255\n",
      "Epoch 131, average loss 0.7955128411883808\n",
      "Epoch 132, average loss 0.7904878029584753\n",
      "Epoch 133, average loss 0.7931727739818621\n",
      "Epoch 134, average loss 0.7960140026968582\n",
      "Epoch 135, average loss 0.7951404632201078\n",
      "Epoch 136, average loss 0.7884103636704938\n",
      "Epoch 137, average loss 0.7990636017179373\n",
      "Epoch 138, average loss 0.7902745709954937\n",
      "Epoch 139, average loss 0.7864824335171878\n",
      "Epoch 140, average loss 0.7887277685966424\n",
      "Epoch 141, average loss 0.7989394400068897\n",
      "Epoch 142, average loss 0.7876852512806723\n",
      "Epoch 143, average loss 0.7902920614070965\n",
      "Epoch 144, average loss 0.7867858484294453\n",
      "Epoch 145, average loss 0.7885512195646162\n",
      "Epoch 146, average loss 0.7917438702951964\n",
      "Epoch 147, average loss 0.7899736905081391\n",
      "Epoch 148, average loss 0.7872469916281878\n",
      "Epoch 149, average loss 0.785476936411269\n",
      "Epoch 150, average loss 0.7913645702390388\n",
      "Epoch 151, average loss 0.7930734399666184\n",
      "Epoch 152, average loss 0.7873913237685886\n",
      "Epoch 153, average loss 0.7809512445146819\n",
      "Epoch 154, average loss 0.7931959343056649\n",
      "Epoch 155, average loss 0.7876675935675825\n",
      "Epoch 156, average loss 0.7931937607200982\n",
      "Epoch 157, average loss 0.7901685830280004\n",
      "Epoch 158, average loss 0.7871955668291991\n",
      "Epoch 159, average loss 0.7844367819498298\n",
      "Epoch 160, average loss 0.78736911262521\n",
      "Epoch 161, average loss 0.785107405607631\n",
      "Epoch 162, average loss 0.7910434708357651\n",
      "Epoch 163, average loss 0.7843200291085257\n",
      "Epoch 164, average loss 0.7922633536238571\n",
      "Epoch 165, average loss 0.7840609734069215\n",
      "Epoch 166, average loss 0.7870596599222889\n",
      "Epoch 167, average loss 0.7818903512970339\n",
      "Epoch 168, average loss 0.7839473478845501\n",
      "Epoch 169, average loss 0.7841936812353315\n",
      "Epoch 170, average loss 0.7898897124226045\n",
      "Epoch 171, average loss 0.7867638694144999\n",
      "Epoch 172, average loss 0.7902872739507831\n",
      "Epoch 173, average loss 0.7852084618431922\n",
      "Epoch 174, average loss 0.7951404968864089\n",
      "Epoch 175, average loss 0.7909170945250165\n",
      "Epoch 176, average loss 0.7924983728051378\n",
      "Epoch 177, average loss 0.7947916586497206\n",
      "Epoch 178, average loss 0.7885960219097692\n",
      "Epoch 179, average loss 0.7910493658145817\n",
      "Epoch 180, average loss 0.7873649820987554\n",
      "Epoch 181, average loss 0.7859738286867434\n",
      "Epoch 182, average loss 0.7956345386452358\n",
      "Epoch 183, average loss 0.7897965375731952\n",
      "Epoch 184, average loss 0.7895387108155666\n",
      "Epoch 185, average loss 0.7854752623710552\n",
      "Epoch 186, average loss 0.7893324585378622\n",
      "Epoch 187, average loss 0.7832117056747027\n",
      "Epoch 188, average loss 0.7916927525783717\n",
      "Epoch 189, average loss 0.7879792811949108\n",
      "Epoch 190, average loss 0.7941549446475441\n",
      "Epoch 191, average loss 0.7855991422851741\n",
      "Epoch 192, average loss 0.787153409419611\n",
      "Epoch 193, average loss 0.7865349132664867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, average loss 0.7832257090784779\n",
      "Epoch 195, average loss 0.786331079627466\n",
      "Epoch 196, average loss 0.7938841522817406\n",
      "Epoch 197, average loss 0.7845998237139133\n",
      "Epoch 198, average loss 0.7803206183252858\n",
      "Epoch 199, average loss 0.7897082345053781\n",
      "Epoch 200, average loss 0.7929055820761808\n",
      "Epoch 201, average loss 0.7894570654789108\n",
      "Epoch 202, average loss 0.7891576716635792\n",
      "Epoch 203, average loss 0.7808711559930075\n",
      "Epoch 204, average loss 0.7894891536860409\n",
      "Epoch 205, average loss 0.7885004435390722\n",
      "Epoch 206, average loss 0.7876092165136849\n",
      "Epoch 207, average loss 0.7956322102916077\n",
      "Epoch 208, average loss 0.7897322671523145\n",
      "Epoch 209, average loss 0.7888535578645524\n",
      "Epoch 210, average loss 0.7886886607961009\n",
      "Epoch 211, average loss 0.7885704605074007\n",
      "Epoch 212, average loss 0.7819584751378315\n",
      "Epoch 213, average loss 0.7871884845330475\n",
      "Epoch 214, average loss 0.7872915072495374\n",
      "Epoch 215, average loss 0.7854169476886261\n",
      "Epoch 216, average loss 0.778942772075635\n",
      "Epoch 217, average loss 0.7860663324469301\n",
      "Epoch 218, average loss 0.7871181106925773\n",
      "Epoch 219, average loss 0.7818121466494107\n",
      "Epoch 220, average loss 0.7880314129551592\n",
      "Epoch 221, average loss 0.7846390937441424\n",
      "Epoch 222, average loss 0.7890359702621295\n",
      "Epoch 223, average loss 0.7869738132132696\n",
      "Epoch 224, average loss 0.7865715781928122\n",
      "Epoch 225, average loss 0.7811365784944596\n",
      "Epoch 226, average loss 0.7904786465504279\n",
      "Epoch 227, average loss 0.7893839715158072\n",
      "Epoch 228, average loss 0.787531795507684\n",
      "Epoch 229, average loss 0.7880204715976448\n",
      "Epoch 230, average loss 0.7813505223586753\n",
      "Epoch 231, average loss 0.7873863551002237\n",
      "Epoch 232, average loss 0.7813888526796653\n",
      "Epoch 233, average loss 0.7852212731199877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-436-b0c078cf1de3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "net.train()\n",
    "\n",
    "#Save model each iteration of training\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    iters = 0\n",
    "    for i in range(0, train_len, batch_size):\n",
    "        batch = train_set[i:i+batch_size]\n",
    "        \n",
    "        inputs = np.array([j for j,k in batch])\n",
    "        inputs = torch.from_numpy(inputs[:, None, :]).to(device)\n",
    "        \n",
    "        labels = torch.from_numpy(np.array([k for j,k in batch])).type(torch.LongTensor).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        iters += 1\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch}, average loss {running_loss/iters}')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(), './sleep_deprivation_net.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "f74a4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_data, model):\n",
    "    outputs = model(test_data)\n",
    "    \n",
    "    return torch.argmax(outputs, dim=1)\n",
    "\n",
    "def compute_accuracy(inputs, labels, model):\n",
    "    outputs = classify(inputs, model)\n",
    "    \n",
    "    accuracy = (outputs == labels).sum()/outputs.shape[0]\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def print_accuracy(test_set, model):\n",
    "    inputs = np.array([i for i,j in test_set])\n",
    "    inputs = torch.from_numpy(inputs[:, None, :]).to(device)\n",
    "    \n",
    "    labels = torch.from_numpy(np.array([j for i,j in test_set])).to(device)\n",
    "    \n",
    "    accuracy = compute_accuracy(inputs, labels, model)\n",
    "    \n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "3c63c2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Run trained NN on test data and obtain accuracies\n",
    "net.eval()\n",
    "print_accuracy(test_set, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4336ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a28a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
