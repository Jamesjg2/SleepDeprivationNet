{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "291a4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d04518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant variables\n",
    "data_path = '../ds000201-download/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "3824529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and preprocess\n",
    "def load_subject_and_compress(subject_name, task_name, data_shape, data_path):\n",
    "    subject_path = data_path + subject_name\n",
    "    \n",
    "    ses_1_data_path = subject_path + f'/ses-1/func/{subject_name}_ses-1_{task_name}.nii.gz'\n",
    "    ses_2_data_path = subject_path + f'/ses-2/func/{subject_name}_ses-2_{task_name}.nii.gz'\n",
    "    \n",
    "    print(ses_1_data_path)\n",
    "    try:\n",
    "        ses_1_img = nib.load(ses_1_data_path)\n",
    "        ses_2_img = nib.load(ses_2_data_path)\n",
    "    except(FileNotFoundError):\n",
    "        return None\n",
    "    \n",
    "    ses_1_data = ses_1_img.get_fdata()\n",
    "    ses_2_data = ses_2_img.get_fdata()\n",
    "    \n",
    "    #Average along time axis to compress data\n",
    "    ses_1 = np.mean(ses_1_data, axis=-1)\n",
    "    ses_1 = np.pad(ses_1, [(0, data_shape[0] - ses_1_data.shape[0]), \n",
    "                           (0, data_shape[1] - ses_1_data.shape[1]), \n",
    "                           (0, data_shape[2] - ses_1_data.shape[2])])\n",
    "    \n",
    "    ses_2 = np.mean(ses_2_data, axis=-1)\n",
    "    ses_2 = np.pad(ses_2, [(0, data_shape[0] - ses_2_data.shape[0]), \n",
    "                           (0, data_shape[1] - ses_2_data.shape[1]), \n",
    "                           (0, data_shape[2] - ses_2_data.shape[2])])\n",
    "    \n",
    "    # Normalize\n",
    "    \n",
    "    ses_1 = (ses_1 - np.mean(ses_1)) / np.std(ses_1)\n",
    "    ses_2 = (ses_2 - np.mean(ses_2)) / np.std(ses_2)\n",
    "    \n",
    "    return ses_1, ses_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "a3d33e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the compressed subject data for all valid subjects\n",
    "def retrieve_data(data_path, task_name, data_shape):\n",
    "    ses_1_data = []\n",
    "    ses_2_data = []\n",
    "    \n",
    "    for i in range(9001, 9101):\n",
    "        compressed_subject_data = load_subject_and_compress(f'sub-{i}', task_name, data_shape, data_path)\n",
    "        if (compressed_subject_data != None):\n",
    "            print(compressed_subject_data[0].shape)\n",
    "            print(compressed_subject_data[1].shape)\n",
    "            ses_1_data.append(compressed_subject_data[0])\n",
    "            ses_2_data.append(compressed_subject_data[1])\n",
    "            \n",
    "#     for root, dirs, files in os.walk(data_path):\n",
    "#         for folder_name in dirs:\n",
    "#             print(folder_name)\n",
    "#             if folder_name[:3] == 'sub' and len(folder_name) == 8:\n",
    "#                 compressed_subject_data = load_subject_and_compress(folder_name, data_path)\n",
    "#                 if (compressed_subject_data != None):\n",
    "#                     ses_1_data.append(compressed_subject_data[0])\n",
    "#                     ses_2_data.append(compressed_subject_data[1]) \n",
    "    \n",
    "    return np.array(ses_1_data), np.array(ses_2_data)\n",
    "\n",
    "# Split data into training and test sets\n",
    "def split_training_test(ses_1_data, ses_2_data):\n",
    "    # 0 label means rested, 1 label means sleepy\n",
    "    assert(ses_1_data.shape == ses_2_data.shape)\n",
    "    \n",
    "    N = ses_1_data.shape[0]\n",
    "    \n",
    "    shuffler = np.random.permutation(N)\n",
    "    ses_1_shuffle = ses_1_data[shuffler]\n",
    "    ses_2_shuffle = ses_2_data[shuffler]\n",
    "    \n",
    "    num_train = int(.8 * N)\n",
    "    num_test = N - num_train\n",
    "    \n",
    "    #Split into training and test sets, keeping subjects consistent within sets\n",
    "    ses_1_train = ses_1_shuffle[:num_train]\n",
    "    ses_1_test = ses_1_shuffle[-num_test:]\n",
    "    ses_2_train = ses_2_shuffle[:num_train]\n",
    "    ses_2_test = ses_2_shuffle[-num_test:]\n",
    "    \n",
    "    #Define labels for training and test sets\n",
    "    ses_1_train_labels = [0 for i in range(num_train)]\n",
    "    ses_1_test_labels = [0 for i in range(num_test)]\n",
    "    ses_2_train_labels = [1 for i in range(num_train)]\n",
    "    ses_2_test_labels = [1 for i in range(num_train)]\n",
    "    \n",
    "    #zip labels up\n",
    "    ses_1_train_set = zip(ses_1_train, ses_1_train_labels)\n",
    "    ses_2_train_set = zip(ses_2_train, ses_2_train_labels)\n",
    "    ses_1_test_set = zip(ses_1_test, ses_1_test_labels)\n",
    "    ses_2_test_set =zip(ses_2_test, ses_2_test_labels)\n",
    "    \n",
    "    #put all training and test fMRI images into the same train set, with labels included now\n",
    "    train_set = list(ses_1_train_set) + list(ses_2_train_set)\n",
    "    test_set = list(ses_1_test_set) + list(ses_2_test_set)\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "5f07b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ds000201-download/sub-9001/ses-1/func/sub-9001_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9002/ses-1/func/sub-9002_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9003/ses-1/func/sub-9003_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9004/ses-1/func/sub-9004_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9005/ses-1/func/sub-9005_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9006/ses-1/func/sub-9006_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9007/ses-1/func/sub-9007_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9008/ses-1/func/sub-9008_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9009/ses-1/func/sub-9009_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9010/ses-1/func/sub-9010_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9011/ses-1/func/sub-9011_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9012/ses-1/func/sub-9012_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9013/ses-1/func/sub-9013_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9014/ses-1/func/sub-9014_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9015/ses-1/func/sub-9015_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9016/ses-1/func/sub-9016_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9017/ses-1/func/sub-9017_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9018/ses-1/func/sub-9018_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9019/ses-1/func/sub-9019_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9020/ses-1/func/sub-9020_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9021/ses-1/func/sub-9021_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9022/ses-1/func/sub-9022_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9023/ses-1/func/sub-9023_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9024/ses-1/func/sub-9024_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9025/ses-1/func/sub-9025_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9026/ses-1/func/sub-9026_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9027/ses-1/func/sub-9027_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9028/ses-1/func/sub-9028_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9029/ses-1/func/sub-9029_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9030/ses-1/func/sub-9030_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9031/ses-1/func/sub-9031_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9032/ses-1/func/sub-9032_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9033/ses-1/func/sub-9033_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9034/ses-1/func/sub-9034_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9035/ses-1/func/sub-9035_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9036/ses-1/func/sub-9036_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9037/ses-1/func/sub-9037_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9038/ses-1/func/sub-9038_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9039/ses-1/func/sub-9039_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9040/ses-1/func/sub-9040_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9041/ses-1/func/sub-9041_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9042/ses-1/func/sub-9042_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9043/ses-1/func/sub-9043_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9044/ses-1/func/sub-9044_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9045/ses-1/func/sub-9045_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9046/ses-1/func/sub-9046_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9047/ses-1/func/sub-9047_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9048/ses-1/func/sub-9048_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9049/ses-1/func/sub-9049_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9050/ses-1/func/sub-9050_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9051/ses-1/func/sub-9051_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9052/ses-1/func/sub-9052_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9053/ses-1/func/sub-9053_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9054/ses-1/func/sub-9054_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9055/ses-1/func/sub-9055_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9056/ses-1/func/sub-9056_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9057/ses-1/func/sub-9057_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9058/ses-1/func/sub-9058_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9059/ses-1/func/sub-9059_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9060/ses-1/func/sub-9060_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9061/ses-1/func/sub-9061_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9062/ses-1/func/sub-9062_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9063/ses-1/func/sub-9063_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9064/ses-1/func/sub-9064_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9065/ses-1/func/sub-9065_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9066/ses-1/func/sub-9066_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9067/ses-1/func/sub-9067_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9068/ses-1/func/sub-9068_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9069/ses-1/func/sub-9069_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9070/ses-1/func/sub-9070_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9071/ses-1/func/sub-9071_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9072/ses-1/func/sub-9072_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9073/ses-1/func/sub-9073_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9074/ses-1/func/sub-9074_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9075/ses-1/func/sub-9075_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9076/ses-1/func/sub-9076_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9077/ses-1/func/sub-9077_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9078/ses-1/func/sub-9078_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9079/ses-1/func/sub-9079_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9080/ses-1/func/sub-9080_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9081/ses-1/func/sub-9081_ses-1_task-rest_bold.nii.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9082/ses-1/func/sub-9082_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9083/ses-1/func/sub-9083_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9084/ses-1/func/sub-9084_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9085/ses-1/func/sub-9085_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9086/ses-1/func/sub-9086_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9087/ses-1/func/sub-9087_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9088/ses-1/func/sub-9088_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9089/ses-1/func/sub-9089_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9090/ses-1/func/sub-9090_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9091/ses-1/func/sub-9091_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9092/ses-1/func/sub-9092_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9093/ses-1/func/sub-9093_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9094/ses-1/func/sub-9094_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9095/ses-1/func/sub-9095_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9096/ses-1/func/sub-9096_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9097/ses-1/func/sub-9097_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9098/ses-1/func/sub-9098_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n",
      "../ds000201-download/sub-9099/ses-1/func/sub-9099_ses-1_task-rest_bold.nii.gz\n",
      "../ds000201-download/sub-9100/ses-1/func/sub-9100_ses-1_task-rest_bold.nii.gz\n",
      "(128, 128, 49)\n",
      "(128, 128, 49)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve rest data\n",
    "ses_1_rest_task_data, ses_2_rest_task_data = retrieve_data(data_path, 'task-rest_bold', data_shape=(128,128,49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "23c4a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDeprivationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 8, 7, stride = 2)\n",
    "        self.conv2 = nn.Conv3d(1, 16, 5, stride = 2)\n",
    "        self.conv3 = nn.Conv3d(1, 32, 3, stride = 2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(25088,  2048)\n",
    "        self.linear2 = nn.Linear(2048, 128)\n",
    "        self.linear3 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = x[:, None, :]\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = x[:, None, :]\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.linear1(self.dropout(torch.flatten(x, start_dim=1)))))\n",
    "        x = self.dropout(F.relu(self.linear2(x)))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "49c96195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate net\n",
    "net = SleepDeprivationNet()\n",
    "net = net.double()\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "2607331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128, 49])\n",
      "torch.Size([1, 1, 128, 128, 49])\n"
     ]
    }
   ],
   "source": [
    "# Simple sanity checks \n",
    "test = torch.from_numpy(ses_1_train[0])\n",
    "label = torch.tensor([0]).to(device)\n",
    "print(test.shape)\n",
    "test = test[None, None, :]\n",
    "print(test.shape)\n",
    "\n",
    "test = test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "b0fa44e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.7684, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.5547, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3156, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.5118, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3301, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3535, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3142, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3147, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3612, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3145, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3134, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3164, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3137, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3143, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.3142, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3153, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "loss tensor(0.3133, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Overfit one sample for sanity\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "num_epochs = 100\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min = 1e-6)\n",
    "\n",
    "net.train()\n",
    "\n",
    "#Save model each iteration of training\n",
    "for epoch in range(num_epochs):     \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(test)\n",
    "    \n",
    "    loss = criterion(outputs, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss\", loss)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "869b0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 3.6974e-08]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "35e61ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "train_set, test_set = split_training_test(ses_1_rest_task_data, ses_2_rest_task_data)\n",
    "\n",
    "#Shuffle train and test data\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "e762a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3)\n",
    "num_epochs = 10000\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min = 1e-6)\n",
    "                                                                                                 \n",
    "batch_size = 8\n",
    "train_len = len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss 0.6930998087534775\n",
      "Epoch 1, average loss 0.6931362178908568\n",
      "Epoch 2, average loss 0.692919148821814\n",
      "Epoch 3, average loss 0.6928898914867372\n",
      "Epoch 4, average loss 0.6933186761570876\n",
      "Epoch 5, average loss 0.6929547467276119\n",
      "Epoch 6, average loss 0.6929960583778279\n",
      "Epoch 7, average loss 0.6928863101019779\n",
      "Epoch 8, average loss 0.6933204120350769\n",
      "Epoch 9, average loss 0.6932885091946059\n",
      "Epoch 10, average loss 0.6930540472573655\n",
      "Epoch 11, average loss 0.6937791561143308\n",
      "Epoch 12, average loss 0.6932995912187816\n",
      "Epoch 13, average loss 0.6934602418568468\n",
      "Epoch 14, average loss 0.6935823071278352\n",
      "Epoch 15, average loss 0.6932619190149473\n",
      "Epoch 16, average loss 0.6932975734146806\n",
      "Epoch 17, average loss 0.6932426482711141\n",
      "Epoch 18, average loss 0.6932334794749014\n",
      "Epoch 19, average loss 0.693505065846261\n",
      "Epoch 20, average loss 0.6931633755661067\n",
      "Epoch 21, average loss 0.693090069171525\n",
      "Epoch 22, average loss 0.6928344940563419\n",
      "Epoch 23, average loss 0.6931130301781496\n",
      "Epoch 24, average loss 0.6935590221045038\n",
      "Epoch 25, average loss 0.6932510608808218\n",
      "Epoch 26, average loss 0.69278522509145\n",
      "Epoch 27, average loss 0.6928722754955224\n",
      "Epoch 28, average loss 0.6935791782234139\n",
      "Epoch 29, average loss 0.6928052613886969\n",
      "Epoch 30, average loss 0.6930301179901216\n",
      "Epoch 31, average loss 0.693154289686499\n",
      "Epoch 32, average loss 0.6928992924574375\n",
      "Epoch 33, average loss 0.6931597361536066\n",
      "Epoch 34, average loss 0.692966795242784\n",
      "Epoch 35, average loss 0.6929892020754499\n",
      "Epoch 36, average loss 0.6931010422762468\n",
      "Epoch 37, average loss 0.6933984471993863\n",
      "Epoch 38, average loss 0.6930830556712123\n",
      "Epoch 39, average loss 0.693309719839721\n",
      "Epoch 40, average loss 0.6934877224573568\n",
      "Epoch 41, average loss 0.6931004545715032\n",
      "Epoch 42, average loss 0.6928498276135391\n",
      "Epoch 43, average loss 0.6932175796450548\n",
      "Epoch 44, average loss 0.6931708640413989\n",
      "Epoch 45, average loss 0.693847004984034\n",
      "Epoch 46, average loss 0.6935736294642927\n",
      "Epoch 47, average loss 0.6932838379134346\n",
      "Epoch 48, average loss 0.6931968761906584\n",
      "Epoch 49, average loss 0.6932077138620022\n",
      "Epoch 50, average loss 0.6931986969854456\n",
      "Epoch 51, average loss 0.6933236979052014\n",
      "Epoch 52, average loss 0.6933684713619201\n",
      "Epoch 53, average loss 0.6928452166863142\n",
      "Epoch 54, average loss 0.6929799482678025\n",
      "Epoch 55, average loss 0.6931092866479569\n",
      "Epoch 56, average loss 0.6937062564449604\n",
      "Epoch 57, average loss 0.6933255035397207\n",
      "Epoch 58, average loss 0.6929962326095558\n",
      "Epoch 59, average loss 0.6927765752164585\n",
      "Epoch 60, average loss 0.6933645008369013\n",
      "Epoch 61, average loss 0.6933223490419976\n",
      "Epoch 62, average loss 0.6924236086144917\n",
      "Epoch 63, average loss 0.6933441161921999\n",
      "Epoch 64, average loss 0.6929654701562898\n",
      "Epoch 65, average loss 0.6929638090569563\n",
      "Epoch 66, average loss 0.6932373198389825\n",
      "Epoch 67, average loss 0.6935340097342203\n",
      "Epoch 68, average loss 0.69327078787782\n",
      "Epoch 69, average loss 0.6931173319231136\n",
      "Epoch 70, average loss 0.693158230833562\n",
      "Epoch 71, average loss 0.6934123034493125\n",
      "Epoch 72, average loss 0.6934055022586696\n",
      "Epoch 73, average loss 0.6930130229285837\n",
      "Epoch 74, average loss 0.6926243562344112\n",
      "Epoch 75, average loss 0.6930971067740059\n",
      "Epoch 76, average loss 0.693297996650787\n",
      "Epoch 77, average loss 0.69354536980756\n",
      "Epoch 78, average loss 0.6930263676309768\n",
      "Epoch 79, average loss 0.6934764767168166\n",
      "Epoch 80, average loss 0.6930484259447309\n",
      "Epoch 81, average loss 0.6929291596976775\n",
      "Epoch 82, average loss 0.692753512264175\n",
      "Epoch 83, average loss 0.6934712040218167\n",
      "Epoch 84, average loss 0.6929918033352555\n",
      "Epoch 85, average loss 0.6927954261662492\n",
      "Epoch 86, average loss 0.6931691315520029\n",
      "Epoch 87, average loss 0.6932014301024138\n",
      "Epoch 88, average loss 0.6927471915604082\n",
      "Epoch 89, average loss 0.693415626619749\n",
      "Epoch 90, average loss 0.6928545126575505\n",
      "Epoch 91, average loss 0.6932569390007113\n",
      "Epoch 92, average loss 0.6933000190847323\n",
      "Epoch 93, average loss 0.6928180947074115\n",
      "Epoch 94, average loss 0.6932140053746811\n",
      "Epoch 95, average loss 0.6935431378589959\n",
      "Epoch 96, average loss 0.6927165187379707\n",
      "Epoch 97, average loss 0.6932058965841342\n",
      "Epoch 98, average loss 0.6930872318112204\n",
      "Epoch 99, average loss 0.6931898805386821\n",
      "Epoch 100, average loss 0.692984078683838\n",
      "Epoch 101, average loss 0.6926604699804113\n",
      "Epoch 102, average loss 0.6932635186772659\n",
      "Epoch 103, average loss 0.6933819003695151\n",
      "Epoch 104, average loss 0.6928219868747392\n",
      "Epoch 105, average loss 0.6931193758795635\n",
      "Epoch 106, average loss 0.6931544634163764\n",
      "Epoch 107, average loss 0.6932549940257797\n",
      "Epoch 108, average loss 0.6930598679234725\n",
      "Epoch 109, average loss 0.6934732771937544\n",
      "Epoch 110, average loss 0.6931442701718635\n",
      "Epoch 111, average loss 0.6931511798832107\n",
      "Epoch 112, average loss 0.6934785333778465\n",
      "Epoch 113, average loss 0.6933323827331033\n",
      "Epoch 114, average loss 0.6929938312900384\n",
      "Epoch 115, average loss 0.6931236634112576\n",
      "Epoch 116, average loss 0.693297922308827\n",
      "Epoch 117, average loss 0.6931510899159609\n",
      "Epoch 118, average loss 0.6934388456208076\n",
      "Epoch 119, average loss 0.6930077341765359\n",
      "Epoch 120, average loss 0.693111670230549\n",
      "Epoch 121, average loss 0.6935201205078998\n",
      "Epoch 122, average loss 0.6930434512850288\n",
      "Epoch 123, average loss 0.6925809118342012\n",
      "Epoch 124, average loss 0.6934239181346838\n",
      "Epoch 125, average loss 0.6934216534702847\n",
      "Epoch 126, average loss 0.6936049657419956\n",
      "Epoch 127, average loss 0.6931027769114575\n",
      "Epoch 128, average loss 0.6932288077434551\n",
      "Epoch 129, average loss 0.6927840372654291\n",
      "Epoch 130, average loss 0.6931877370268147\n",
      "Epoch 131, average loss 0.6931818524962248\n",
      "Epoch 132, average loss 0.693203230181065\n",
      "Epoch 133, average loss 0.6932628924563375\n",
      "Epoch 134, average loss 0.6934101231386063\n",
      "Epoch 135, average loss 0.6931015530987671\n",
      "Epoch 136, average loss 0.6929738088906967\n",
      "Epoch 137, average loss 0.6932811544723552\n",
      "Epoch 138, average loss 0.692871742402157\n",
      "Epoch 139, average loss 0.6930062769226041\n",
      "Epoch 140, average loss 0.6926291112654573\n",
      "Epoch 141, average loss 0.6929403069774095\n",
      "Epoch 142, average loss 0.6932135028762136\n",
      "Epoch 143, average loss 0.6931287787775973\n",
      "Epoch 144, average loss 0.6932833159239762\n",
      "Epoch 145, average loss 0.6930163819626514\n",
      "Epoch 146, average loss 0.6928911546423301\n",
      "Epoch 147, average loss 0.6932665491385867\n",
      "Epoch 148, average loss 0.6932635909157021\n",
      "Epoch 149, average loss 0.69314792603244\n",
      "Epoch 150, average loss 0.6928766087780849\n",
      "Epoch 151, average loss 0.6937656282306365\n",
      "Epoch 152, average loss 0.693228138581135\n",
      "Epoch 153, average loss 0.6929640143350303\n",
      "Epoch 154, average loss 0.6934804090996705\n",
      "Epoch 155, average loss 0.6930684627196878\n",
      "Epoch 156, average loss 0.6931688416442894\n",
      "Epoch 157, average loss 0.6934194499996317\n",
      "Epoch 158, average loss 0.6929787140556568\n",
      "Epoch 159, average loss 0.6931099293309467\n",
      "Epoch 160, average loss 0.6928597320859713\n",
      "Epoch 161, average loss 0.693457336484675\n",
      "Epoch 162, average loss 0.6933577486299037\n",
      "Epoch 163, average loss 0.6930659519380912\n",
      "Epoch 164, average loss 0.6932021467496315\n",
      "Epoch 165, average loss 0.6932639125689437\n",
      "Epoch 166, average loss 0.6931895223514098\n",
      "Epoch 167, average loss 0.6931847433548233\n",
      "Epoch 168, average loss 0.6934475277832229\n",
      "Epoch 169, average loss 0.6932548956701977\n",
      "Epoch 170, average loss 0.6931283863385506\n",
      "Epoch 171, average loss 0.6934620345643868\n",
      "Epoch 172, average loss 0.6931050489274238\n",
      "Epoch 173, average loss 0.6931773884107693\n",
      "Epoch 174, average loss 0.6929533706741322\n",
      "Epoch 175, average loss 0.6928185374519363\n",
      "Epoch 176, average loss 0.69307933594631\n",
      "Epoch 177, average loss 0.693005098135439\n",
      "Epoch 178, average loss 0.6933956288733805\n",
      "Epoch 179, average loss 0.6931584065856278\n",
      "Epoch 180, average loss 0.6934078945667774\n",
      "Epoch 181, average loss 0.6931292312508264\n",
      "Epoch 182, average loss 0.6927851697303137\n",
      "Epoch 183, average loss 0.6930956836126668\n",
      "Epoch 184, average loss 0.6933712465633284\n",
      "Epoch 185, average loss 0.6931744329080783\n",
      "Epoch 186, average loss 0.6935092647598093\n",
      "Epoch 187, average loss 0.6930193639240885\n",
      "Epoch 188, average loss 0.6927325014414546\n",
      "Epoch 189, average loss 0.6925884528221871\n",
      "Epoch 190, average loss 0.6932878168899533\n",
      "Epoch 191, average loss 0.6935207737027598\n",
      "Epoch 192, average loss 0.6932108033470991\n",
      "Epoch 193, average loss 0.6935872051416181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, average loss 0.6931830352088043\n",
      "Epoch 195, average loss 0.6931699746325964\n",
      "Epoch 196, average loss 0.6934530036789901\n",
      "Epoch 197, average loss 0.6932026184413028\n",
      "Epoch 198, average loss 0.6931205970198477\n",
      "Epoch 199, average loss 0.6930529403441321\n",
      "Epoch 200, average loss 0.6933320132877341\n",
      "Epoch 201, average loss 0.6932484345568158\n",
      "Epoch 202, average loss 0.6933941023738772\n",
      "Epoch 203, average loss 0.6932218995384094\n",
      "Epoch 204, average loss 0.6931728810871547\n",
      "Epoch 205, average loss 0.6929419252223277\n",
      "Epoch 206, average loss 0.69329990299087\n",
      "Epoch 207, average loss 0.6928570332114283\n",
      "Epoch 208, average loss 0.6930491970871123\n",
      "Epoch 209, average loss 0.6938043152456953\n",
      "Epoch 210, average loss 0.6929208529555346\n",
      "Epoch 211, average loss 0.6931914978158373\n",
      "Epoch 212, average loss 0.6927195743508917\n",
      "Epoch 213, average loss 0.6929136515978345\n",
      "Epoch 214, average loss 0.6932257518303979\n",
      "Epoch 215, average loss 0.6936895200856665\n",
      "Epoch 216, average loss 0.6934143308980663\n",
      "Epoch 217, average loss 0.6934965464906183\n",
      "Epoch 218, average loss 0.6930741492308768\n",
      "Epoch 219, average loss 0.6931048617503893\n",
      "Epoch 220, average loss 0.6928071165806098\n",
      "Epoch 221, average loss 0.6932383696983974\n",
      "Epoch 222, average loss 0.6928514362996654\n",
      "Epoch 223, average loss 0.6930816922936422\n",
      "Epoch 224, average loss 0.693161394049774\n",
      "Epoch 225, average loss 0.6931962363032116\n",
      "Epoch 226, average loss 0.6934265687920124\n",
      "Epoch 227, average loss 0.6932489562362435\n",
      "Epoch 228, average loss 0.6929745341348551\n",
      "Epoch 229, average loss 0.6932867763068423\n",
      "Epoch 230, average loss 0.6931644063567621\n",
      "Epoch 231, average loss 0.6928799670072164\n",
      "Epoch 232, average loss 0.6927759006969432\n",
      "Epoch 233, average loss 0.6925126627770687\n",
      "Epoch 234, average loss 0.6933445444441594\n",
      "Epoch 235, average loss 0.6930807264575227\n",
      "Epoch 236, average loss 0.6932426017970719\n",
      "Epoch 237, average loss 0.6931326237872881\n",
      "Epoch 238, average loss 0.6933723827890472\n",
      "Epoch 239, average loss 0.693382709752752\n",
      "Epoch 240, average loss 0.6935638131758892\n",
      "Epoch 241, average loss 0.6932636653201051\n",
      "Epoch 242, average loss 0.6929471962196999\n",
      "Epoch 243, average loss 0.6930876039947074\n",
      "Epoch 244, average loss 0.6930698065263733\n",
      "Epoch 245, average loss 0.6929469427983631\n",
      "Epoch 246, average loss 0.6933005239488826\n",
      "Epoch 247, average loss 0.6926632189091928\n",
      "Epoch 248, average loss 0.6927396288008998\n",
      "Epoch 249, average loss 0.693527069066992\n",
      "Epoch 250, average loss 0.6932765353575864\n",
      "Epoch 251, average loss 0.6933576401177562\n",
      "Epoch 252, average loss 0.6931573732884363\n",
      "Epoch 253, average loss 0.6928035102245904\n",
      "Epoch 254, average loss 0.6930459907785286\n",
      "Epoch 255, average loss 0.6928910192861566\n",
      "Epoch 256, average loss 0.6928478198891302\n",
      "Epoch 257, average loss 0.6935368420830452\n",
      "Epoch 258, average loss 0.6929447370541386\n",
      "Epoch 259, average loss 0.6932353182039879\n",
      "Epoch 260, average loss 0.692767484890228\n",
      "Epoch 261, average loss 0.6930909321527003\n",
      "Epoch 262, average loss 0.6933259937676626\n",
      "Epoch 263, average loss 0.6929864020545302\n",
      "Epoch 264, average loss 0.6929752194553227\n",
      "Epoch 265, average loss 0.6932443363495515\n",
      "Epoch 266, average loss 0.6930884686542784\n",
      "Epoch 267, average loss 0.6933804725662976\n",
      "Epoch 268, average loss 0.6929726116252272\n",
      "Epoch 269, average loss 0.6931124506327997\n",
      "Epoch 270, average loss 0.6931424684442284\n",
      "Epoch 271, average loss 0.6929792182854462\n",
      "Epoch 272, average loss 0.6934311106742962\n",
      "Epoch 273, average loss 0.6930574467874824\n",
      "Epoch 274, average loss 0.6930019252443004\n",
      "Epoch 275, average loss 0.6928658183651012\n",
      "Epoch 276, average loss 0.693066957610892\n",
      "Epoch 277, average loss 0.6933138027599745\n",
      "Epoch 278, average loss 0.6931392875047707\n",
      "Epoch 279, average loss 0.6932429858901104\n",
      "Epoch 280, average loss 0.6931041449846533\n",
      "Epoch 281, average loss 0.6928968364725674\n",
      "Epoch 282, average loss 0.6930551563240227\n",
      "Epoch 283, average loss 0.6932457454620978\n",
      "Epoch 284, average loss 0.6932610246031896\n",
      "Epoch 285, average loss 0.6934298048810493\n",
      "Epoch 286, average loss 0.6930187027711413\n",
      "Epoch 287, average loss 0.6928599137730672\n",
      "Epoch 288, average loss 0.6932426034421364\n",
      "Epoch 289, average loss 0.6930724002385205\n",
      "Epoch 290, average loss 0.6930146505785371\n",
      "Epoch 291, average loss 0.6928819016470641\n",
      "Epoch 292, average loss 0.6929367235289553\n",
      "Epoch 293, average loss 0.6931785576215991\n",
      "Epoch 294, average loss 0.6932204884490771\n",
      "Epoch 295, average loss 0.6934175501074739\n",
      "Epoch 296, average loss 0.6929582037566067\n",
      "Epoch 297, average loss 0.6932443775524927\n",
      "Epoch 298, average loss 0.6929965069848206\n",
      "Epoch 299, average loss 0.6933993563619651\n",
      "Epoch 300, average loss 0.6931238344378687\n",
      "Epoch 301, average loss 0.6930611444579482\n",
      "Epoch 302, average loss 0.6934501955982086\n",
      "Epoch 303, average loss 0.6932408711743943\n",
      "Epoch 304, average loss 0.6930975563477958\n",
      "Epoch 305, average loss 0.6931049863745367\n",
      "Epoch 306, average loss 0.6930387838460816\n",
      "Epoch 307, average loss 0.6931124022681162\n",
      "Epoch 308, average loss 0.6937955515069633\n",
      "Epoch 309, average loss 0.6929491275261104\n",
      "Epoch 310, average loss 0.6928277882885191\n",
      "Epoch 311, average loss 0.6932196360917486\n",
      "Epoch 312, average loss 0.6934450089135585\n",
      "Epoch 313, average loss 0.6930507101092548\n",
      "Epoch 314, average loss 0.6929813013947183\n",
      "Epoch 315, average loss 0.6931075201161978\n",
      "Epoch 316, average loss 0.693243068232433\n",
      "Epoch 317, average loss 0.6932519392769684\n",
      "Epoch 318, average loss 0.6932678452146531\n",
      "Epoch 319, average loss 0.6930857061832724\n",
      "Epoch 320, average loss 0.6931336740459194\n",
      "Epoch 321, average loss 0.6930891330442865\n",
      "Epoch 322, average loss 0.6930414579784105\n",
      "Epoch 323, average loss 0.6928346192074001\n",
      "Epoch 324, average loss 0.6933985353252262\n",
      "Epoch 325, average loss 0.6929580713523766\n",
      "Epoch 326, average loss 0.6927151229127938\n",
      "Epoch 327, average loss 0.6933655746861807\n",
      "Epoch 328, average loss 0.6934554480133208\n",
      "Epoch 329, average loss 0.693235248156242\n",
      "Epoch 330, average loss 0.6931614173459673\n",
      "Epoch 331, average loss 0.6926894670044641\n",
      "Epoch 332, average loss 0.6936752591342462\n",
      "Epoch 333, average loss 0.6932441800223976\n",
      "Epoch 334, average loss 0.6930467652366548\n",
      "Epoch 335, average loss 0.6933426233717759\n",
      "Epoch 336, average loss 0.6931620952698673\n",
      "Epoch 337, average loss 0.6931984450221269\n",
      "Epoch 338, average loss 0.6932350284043893\n",
      "Epoch 339, average loss 0.6930523204861889\n",
      "Epoch 340, average loss 0.6931064436292499\n",
      "Epoch 341, average loss 0.6931535121301812\n",
      "Epoch 342, average loss 0.6933655880211674\n",
      "Epoch 343, average loss 0.6928617095271908\n",
      "Epoch 344, average loss 0.6933804239981677\n",
      "Epoch 345, average loss 0.6924136991695866\n",
      "Epoch 346, average loss 0.6932557859780417\n",
      "Epoch 347, average loss 0.6931824095551243\n",
      "Epoch 348, average loss 0.6932832276402188\n",
      "Epoch 349, average loss 0.6927821557665439\n",
      "Epoch 350, average loss 0.6933305564267326\n",
      "Epoch 351, average loss 0.6929617520829391\n",
      "Epoch 352, average loss 0.6932876443300386\n",
      "Epoch 353, average loss 0.6936817942786632\n",
      "Epoch 354, average loss 0.6930173491386281\n",
      "Epoch 355, average loss 0.6933193457982036\n",
      "Epoch 356, average loss 0.6931141609523883\n",
      "Epoch 357, average loss 0.6929933403844808\n",
      "Epoch 358, average loss 0.6933814750040264\n",
      "Epoch 359, average loss 0.6936458199289431\n",
      "Epoch 360, average loss 0.6932522257012832\n",
      "Epoch 361, average loss 0.6931128298928743\n",
      "Epoch 362, average loss 0.6934704849551254\n",
      "Epoch 363, average loss 0.6930220048327256\n",
      "Epoch 364, average loss 0.6930987270400266\n",
      "Epoch 365, average loss 0.6932682333935363\n",
      "Epoch 366, average loss 0.6931523442152648\n",
      "Epoch 367, average loss 0.6930715104302558\n",
      "Epoch 368, average loss 0.6934401805640096\n",
      "Epoch 369, average loss 0.693027708385707\n",
      "Epoch 370, average loss 0.6930713977361076\n",
      "Epoch 371, average loss 0.6925519634850673\n",
      "Epoch 372, average loss 0.6928139706175319\n",
      "Epoch 373, average loss 0.6931074300357789\n",
      "Epoch 374, average loss 0.6932624053576998\n",
      "Epoch 375, average loss 0.6930640045716128\n",
      "Epoch 376, average loss 0.6931850849048732\n",
      "Epoch 377, average loss 0.6929445301195641\n",
      "Epoch 378, average loss 0.6929081508442099\n",
      "Epoch 379, average loss 0.6934831204764331\n",
      "Epoch 380, average loss 0.6930216998149751\n",
      "Epoch 381, average loss 0.6931218185920787\n",
      "Epoch 382, average loss 0.6930774234042613\n",
      "Epoch 383, average loss 0.6931734968493943\n",
      "Epoch 384, average loss 0.6930640373345738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385, average loss 0.6928141296802084\n",
      "Epoch 386, average loss 0.69347266085767\n",
      "Epoch 387, average loss 0.6933780650272743\n",
      "Epoch 388, average loss 0.693467533394263\n",
      "Epoch 389, average loss 0.6931670238608039\n",
      "Epoch 390, average loss 0.6931482854844309\n",
      "Epoch 391, average loss 0.6929968376758017\n",
      "Epoch 392, average loss 0.6930704813230192\n",
      "Epoch 393, average loss 0.6928188284850396\n",
      "Epoch 394, average loss 0.6932483421409347\n",
      "Epoch 395, average loss 0.6931969735168844\n",
      "Epoch 396, average loss 0.6932106209167352\n",
      "Epoch 397, average loss 0.6936034141729246\n",
      "Epoch 398, average loss 0.6931705538993306\n",
      "Epoch 399, average loss 0.693380243278784\n",
      "Epoch 400, average loss 0.6928301047162959\n",
      "Epoch 401, average loss 0.6934921213378848\n",
      "Epoch 402, average loss 0.6929120348703901\n",
      "Epoch 403, average loss 0.6932970164330188\n",
      "Epoch 404, average loss 0.6935728718280313\n",
      "Epoch 405, average loss 0.6932824007893212\n",
      "Epoch 406, average loss 0.6931892669412236\n",
      "Epoch 407, average loss 0.6930434784298029\n",
      "Epoch 408, average loss 0.693406856596568\n",
      "Epoch 409, average loss 0.6925143368146387\n",
      "Epoch 410, average loss 0.6931919274536797\n",
      "Epoch 411, average loss 0.6933221132038183\n",
      "Epoch 412, average loss 0.6933580370521187\n",
      "Epoch 413, average loss 0.6931993016584886\n",
      "Epoch 414, average loss 0.6929835666727856\n",
      "Epoch 415, average loss 0.6929251312921171\n",
      "Epoch 416, average loss 0.6934292622359143\n",
      "Epoch 417, average loss 0.6932774180696049\n",
      "Epoch 418, average loss 0.6932449605202474\n",
      "Epoch 419, average loss 0.6932799182772593\n",
      "Epoch 420, average loss 0.6929127561356418\n",
      "Epoch 421, average loss 0.6930860741925411\n",
      "Epoch 422, average loss 0.6930341332237341\n",
      "Epoch 423, average loss 0.6929799997077579\n",
      "Epoch 424, average loss 0.6932102881638914\n",
      "Epoch 425, average loss 0.693012512297329\n",
      "Epoch 426, average loss 0.6930106409969782\n",
      "Epoch 427, average loss 0.6933459806489687\n",
      "Epoch 428, average loss 0.6934008674930133\n",
      "Epoch 429, average loss 0.6928547241552958\n",
      "Epoch 430, average loss 0.6931734448547955\n",
      "Epoch 431, average loss 0.6934985436018909\n",
      "Epoch 432, average loss 0.6928989914221425\n",
      "Epoch 433, average loss 0.6933093940027506\n",
      "Epoch 434, average loss 0.6934929306922172\n",
      "Epoch 435, average loss 0.6933349392899582\n",
      "Epoch 436, average loss 0.6930392169704345\n",
      "Epoch 437, average loss 0.6932830222618485\n",
      "Epoch 438, average loss 0.6932344736739114\n",
      "Epoch 439, average loss 0.6933603302095762\n",
      "Epoch 440, average loss 0.6928189789017944\n",
      "Epoch 441, average loss 0.6929241416453844\n",
      "Epoch 442, average loss 0.6931011309715156\n",
      "Epoch 443, average loss 0.6931889465175733\n",
      "Epoch 444, average loss 0.693502257417001\n",
      "Epoch 445, average loss 0.69300950886129\n",
      "Epoch 446, average loss 0.6930312905705125\n",
      "Epoch 447, average loss 0.6932400239643183\n",
      "Epoch 448, average loss 0.6930896395853101\n",
      "Epoch 449, average loss 0.6932332260856127\n",
      "Epoch 450, average loss 0.6933141962189758\n",
      "Epoch 451, average loss 0.6934810171856639\n",
      "Epoch 452, average loss 0.6932669558105583\n",
      "Epoch 453, average loss 0.6931988892128302\n",
      "Epoch 454, average loss 0.6927711888248661\n",
      "Epoch 455, average loss 0.6931669578652877\n",
      "Epoch 456, average loss 0.693372314140258\n",
      "Epoch 457, average loss 0.6929014231972469\n",
      "Epoch 458, average loss 0.693570328829675\n",
      "Epoch 459, average loss 0.6930204961511606\n",
      "Epoch 460, average loss 0.6929320842602262\n",
      "Epoch 461, average loss 0.693047420963848\n",
      "Epoch 462, average loss 0.6931947699359833\n",
      "Epoch 463, average loss 0.692934099480415\n",
      "Epoch 464, average loss 0.6931139166830702\n",
      "Epoch 465, average loss 0.6929410659086231\n",
      "Epoch 466, average loss 0.6930660859674893\n",
      "Epoch 467, average loss 0.6929235203487207\n",
      "Epoch 468, average loss 0.692858261375981\n",
      "Epoch 469, average loss 0.693563851870906\n",
      "Epoch 470, average loss 0.6926885957317169\n",
      "Epoch 471, average loss 0.6933654443903202\n",
      "Epoch 472, average loss 0.6927914961006594\n",
      "Epoch 473, average loss 0.6930547704271548\n",
      "Epoch 474, average loss 0.6938116764566525\n",
      "Epoch 475, average loss 0.6931979714636439\n"
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "net.train()\n",
    "\n",
    "#Save model each iteration of training\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_set)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    iters = 0\n",
    "    for i in range(0, train_len, batch_size):\n",
    "        batch = train_set[i:i+batch_size]\n",
    "        \n",
    "        inputs = np.array([j for j,k in batch])\n",
    "        inputs = torch.from_numpy(inputs[:, None, :]).to(device)\n",
    "        \n",
    "        labels = torch.from_numpy(np.array([k for j,k in batch])).type(torch.LongTensor).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss\n",
    "        iters += 1\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch}, average loss {running_loss/iters}')\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(), './sleep_deprivation_net.pt')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "5ff6f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_data, model):\n",
    "    outputs = model(test_data)\n",
    "    \n",
    "    return torch.argmax(outputs, dim=1)\n",
    "\n",
    "def compute_accuracy(test_set, model):\n",
    "    inputs = np.array([i for i,j in test_set])\n",
    "    inputs = torch.from_numpy(inputs[:, None, :]).to(device)\n",
    "    \n",
    "    labels = torch.from_numpy(np.array([j for i,j in test_set])).to(device)\n",
    "    \n",
    "    outputs = classify(inputs, model)\n",
    "    \n",
    "    accuracy = (outputs == labels).sum()/outputs.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "3c63c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trained NN on test data and obtain accuracies\n",
    "net.eval()\n",
    "accuracy = compute_accuracy(test_set, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "6f438aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea4eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
